import os
import json
import traceback
from tqdm import tqdm
import yaml

from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from pathlib import Path

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer, CrossEncoder

from openai import OpenAI


# =========================
# í™˜ê²½/ëª¨ë¸ ì„¤ì •
# =========================
from dotenv import load_dotenv
import os

_ = load_dotenv()

# ----- OpenAI -----
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)
llm_model = "gpt-4o-mini"

# ----- Embedding / Reranker -----
EMBED_MODEL_NAME = "intfloat/multilingual-e5-base"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-v2-m3"

embed_model = SentenceTransformer(EMBED_MODEL_NAME)
reranker = CrossEncoder(RERANKER_MODEL_NAME)

# ----- Elasticsearch -----
ES_USERNAME = os.getenv("ES_USERNAME")
ES_PASSWORD = os.getenv("ES_PASSWORD")
ES_INDEX = "test"

es = Elasticsearch(
    ["https://localhost:9200"],
    basic_auth=(ES_USERNAME, ES_PASSWORD),
    ca_certs="/opt/elasticsearch-8.8.0/config/certs/http_ca.crt"
)

# print("ğŸ·ï¸ Elasticsearch ì •ë³´ :", es.info())


# =========================
# Config ë¡œë“œ
# =========================
CONFIG_PATH = Path("../configs/config.yaml")

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# print("ğŸ·ï¸ config ì •ë³´ :", config)


# =========================
# 1) Chunking
# =========================

# ----- ë¬¸ì ë‹¨ìœ„ ê¸°ë°˜ chunking í•¨ìˆ˜ -----
def chunk_text(
        text: str,
        chunk_size: int = 700,
        chunk_overlap: int = 150) -> List[str]:
    
    """
    (1) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ chunk_size ê¸¸ì´ë§Œí¼ ìˆœì°¨ì ìœ¼ë¡œ ìë¥¸ë‹¤.
    (2) ê° chunkëŠ” ì´ì „ chunkì™€ chunk_overlap ë§Œí¼ ê²¹ì¹˜ë„ë¡ êµ¬ì„±í•œë‹¤.
       â†’ ë¬¸ë§¥ ë‹¨ì ˆì„ ì™„í™”í•˜ê³  ê²€ìƒ‰ recallì„ ë†’ì´ê¸° ìœ„í•¨
    (3) ë„ˆë¬´ ì§§ì€ chunk(30ì ë¯¸ë§Œ)ëŠ” ì •ë³´ëŸ‰ì´ ì ì–´ ì œê±°í•œë‹¤.
    (4) ì–¸ì–´ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë¬¸ì ê¸°ì¤€ ë¶„í• ì´ë¯€ë¡œ ë‹¤êµ­ì–´ ë¬¸ì„œì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•œë‹¤.
    """

    if not text:
        return []

    text = text.strip()
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunk = text[start:end].strip()
        if len(chunk) >= 30:
            chunks.append(chunk)
        if end == len(text):
            break
        start = max(0, end - chunk_overlap)

    return chunks

# ----- ë¬¸ì„œ ë‹¨ìœ„ â†’ ì²­í‚¹ ë‹¨ìœ„ ë³€í™˜ í•¨ìˆ˜ -----
def make_chunk_docs(
        docs: List[Dict[str, Any]],
        chunk_size: int = 700,
        chunk_overlap: int = 150) -> List[Dict[str, Any]]:
    
    """
    (1) ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì„œì˜ docidì™€ contentë¥¼ ì½ëŠ”ë‹¤.
    (2) contentë¥¼ chunk_text()ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ chunkë¡œ ìƒì„±í•œë‹¤.
    (3) ê° chunkëŠ” ì›ë¬¸ docidë¥¼ ìœ ì§€í•˜ë©°,
       chunk_id = "{docid}_{ìˆœë²ˆ}" í˜•íƒœë¡œ ê³ ìœ  ì‹ë³„ìë¥¼ ë¶€ì—¬í•œë‹¤.
    (4) ì´í›„ ê²€ìƒ‰, rerank, reference ì„ íƒ ë‹¨ê³„ì—ì„œ
       chunk ë‹¨ìœ„ ê²€ìƒ‰ â†’ doc ë‹¨ìœ„ ì§‘ê³„ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.
    """

    chunked = []
    for d in docs:
        docid = d.get("docid")
        content = d.get("content", "")
        chunks = chunk_text(
            content,
            chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for ci, c in enumerate(chunks):
            chunked.append({
                "docid": docid,                 # ì›ë¬¸ doc id ìœ ì§€
                "chunk_id": f"{docid}_{ci}",     # chunk ì‹ë³„ì
                "content": c                    # ES ê²€ìƒ‰ ëŒ€ìƒì€ chunk content
            })
    return chunked


# =========================
# 2) Multilingual Embedding
# =========================

# ----- E5 ê³„ì—´ì€ passage / query prefix -----
def e5_passage(text: str) -> str:
    return f"passage: {text}"

def e5_query(text: str) -> str:
    return f"query: {text}"

# ----- Embedding ìƒì„± í•¨ìˆ˜ -----
def get_embedding_passages(passages: List[str]) -> List[List[float]]:
    vecs = embed_model.encode(
        [e5_passage(p) for p in passages],
        normalize_embeddings=True,
        show_progress_bar=False
    )
    return vecs.tolist()

def get_embedding_query(query: str) -> List[float]:
    vec = embed_model.encode(
        [e5_query(query)],
        normalize_embeddings=True,
        show_progress_bar=False
    )[0]
    return vec.tolist()

def get_embeddings_in_batches(
        docs: List[Dict[str, Any]],
        batch_size: int = 128) -> List[List[float]]:
    all_vecs = []
    with tqdm(total=len(docs), desc="Embedding", unit="doc") as pbar:
        for i in range(0, len(docs), batch_size):
            batch = docs[i:i + batch_size]
            passages = [doc["content"] for doc in batch]
            vecs = get_embedding_passages(passages)
            all_vecs.extend(vecs)
            pbar.update(len(batch))
            pbar.set_postfix(batch=f"{i}:{i+len(batch)}")
    return all_vecs


# =========================
# 3) ES ì¸ë±ìŠ¤/ìƒ‰ì¸
# =========================

# ----- í•œêµ­ì–´ ë§ì¶¤í˜• ë¶„ì„ê¸° ì„¤ì • -----
settings = {
    "analysis": {
        "analyzer": {
            "nori": {
                "type": "custom",
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",         # ë³µí•©ì–´ë¥¼ ë¶„í•´í•˜ë©´ì„œ ì›í˜•ë„ í•¨ê»˜ ìœ ì§€ (ê²€ìƒ‰ recall â†‘)
                "filter": ["nori_posfilter"]        # í•œêµ­ì–´ í’ˆì‚¬ ê¸°ë°˜ stop filter
            }
        },
        "filter": {
            # ì¡°ì‚¬(J), ì–´ë¯¸(E), êµ¬ë‘ì (S*) ë“± ì˜ë¯¸ ê¸°ì—¬ë„ê°€ ë‚®ì€ í’ˆì‚¬ ì œê±°
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
            }
        }
    }
}

# ----- ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜ -----
mappings = {
    "properties": {
        "docid": {"type": "keyword"},
        "chunk_id": {"type": "keyword"},
        "content": {"type": "text", "analyzer": "nori"},
        "embeddings": {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "cosine"
        }
    }
}

# ----- ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜ -----
def create_es_index(
        index: str,
        settings: Dict[str, Any],
        mappings: Dict[str, Any]) -> None:
    """
    (1) ë™ì¼í•œ ì´ë¦„ì˜ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ
    (2) settings + mappingsë¥¼ ì ìš©í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±
    â†’ ì‹¤í—˜ ë°˜ë³µ ì‹œ í•­ìƒ ë™ì¼í•œ ìƒíƒœì—ì„œ ì‹œì‘í•˜ê¸° ìœ„í•¨
    """
    if es.indices.exists(index=index):
        es.indices.delete(index=index)
    es.indices.create(index=index, settings=settings, mappings=mappings)

# ----- Bulk ìƒ‰ì¸ í•¨ìˆ˜ -----
def bulk_add(
        index: str, 
        docs: List[Dict[str, Any]]) -> Any:
    """
    (1) helpers.bulkë¥¼ ì‚¬ìš©í•´ ëŒ€ëŸ‰ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìƒ‰ì¸
    (2) ê° ë¬¸ì„œëŠ” chunk ë‹¨ìœ„ì´ë©°, content(BM25) + embeddings(Dense)ê°€ í•¨ê»˜ ì €ì¥ë¨
    """
    actions = [{"_index": index, "_source": doc} for doc in docs]
    return helpers.bulk(es, actions)


# =========================
# 4) ê²€ìƒ‰ (BM25 + KNN í›„ë³´ ìƒì„±)
# =========================

# ----- Sparse Retrieval (BM25) -----
def sparse_retrieve(query_str: str, size: int = 50):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    return es.search(index=ES_INDEX, query=query, size=size, sort="_score")

# ----- Dense Retrieval (KNN) -----
def dense_retrieve(query_str: str, size: int = 50, num_candidates: int = 200):
    query_vec = get_embedding_query(query_str)
    knn = {
        "field": "embeddings",
        "query_vector": query_vec,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=ES_INDEX, knn=knn)

# ----- BM25 + KNN ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ (chunk_id ê¸°ì¤€ ì¤‘ë³µ ì—†ì´ í•©ì¹˜ê¸°)-----
def merge_hits(bm25_hits, knn_hits, limit: int = 100) -> List[Dict[str, Any]]:
    merged = {}
    for h in bm25_hits:
        cid = h["_source"].get("chunk_id")
        if cid and cid not in merged:
            merged[cid] = h
    for h in knn_hits:
        cid = h["_source"].get("chunk_id")
        if cid and cid not in merged:
            merged[cid] = h

    # ì´ˆê¸° í›„ë³´ëŠ” ES score ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì¶© ì •ë ¬í•´ì„œ limitë¡œ ìë¦„
    cand = list(merged.values())
    cand.sort(key=lambda x: (x.get("_score", 0.0)), reverse=True)
    return cand[:limit]


# =========================
# 5) Reranker (CrossEncoder)
# =========================

# ----- CrossEncoder ê¸°ë°˜ Rerank í•¨ìˆ˜ -----
def rerank(
        query: str, hits: List[Dict[str, Any]],
        topn: int = 20) -> List[Tuple[float, Dict[str, Any]]]:
    """
    (1) (query, chunk_content) ìŒì„ í›„ë³´ ê°œìˆ˜ë§Œí¼ ìƒì„±
       â†’ CrossEncoderëŠ” queryì™€ passageë¥¼ "í•¨ê»˜" ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ì ìˆ˜ë¥¼ ê³„ì‚°
    (2) reranker.predict(pairs)ë¡œ ê° pairì˜ relevance score ì‚°ì¶œ
       â†’ ì ìˆ˜ ìŠ¤ì¼€ì¼ì€ ëª¨ë¸ ë‚´ë¶€ ê¸°ì¤€ì´ë©°, ES _scoreì™€ ì§ì ‘ ë¹„êµí•˜ì§€ ì•ŠìŒ
    (3) score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ topnë§Œ ë°˜í™˜
       â†’ ì´í›„ ë‹¨ê³„ì—ì„œ docid ë‹¨ìœ„ ì§‘ê³„(select_topk_docids)ì— ì‚¬ìš©
    (4) ë°˜í™˜:
    - [(rerank_score, hit), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ topn)
    - hitì—ëŠ” ì›ë˜ ES hit dictê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë¨ (_source í™œìš© ê°€ëŠ¥)
    """
    pairs = [(query, h["_source"]["content"]) for h in hits]
    if not pairs:
        return []

    scores = reranker.predict(pairs)  # numpy array
    scored = list(zip(scores.tolist(), hits))
    scored.sort(key=lambda x: x[0], reverse=True)
    return scored[:topn]


def select_topk_docids(
        scored_hits: List[Tuple[float, Dict[str, Any]]],
        k_doc: int = 3) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    (1) scored_hitsë¥¼ ìˆœíšŒí•˜ë©° docidë³„ ìµœê³  rerank scoreë¥¼ ê¸°ë¡(best_by_doc)
       â†’ "docì˜ ëŒ€í‘œ ì ìˆ˜ = í•´ë‹¹ docì—ì„œ ê°€ì¥ ê´€ë ¨ ë†’ì€ chunkì˜ ì ìˆ˜"
    (2) docidë³„ë¡œ ìµœê³  ì ìˆ˜ë¥¼ ë§Œë“  chunkë¥¼ referencesë¡œ ì €ì¥(best_chunk_by_doc)
       â†’ ì´í›„ LLMì— ë„˜ê¸¸ reference contextë¡œ ì‚¬ìš©ë¨
    (3) docid ëŒ€í‘œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ k_docê°œ docid ì„ íƒ
    (4) ë°˜í™˜:
       - top_docids: ì„ íƒëœ docid ë¦¬ìŠ¤íŠ¸
       - references: ê° docidì—ì„œ ìµœê³  ì ìˆ˜ chunkì˜ {score, content, chunk_id} ì •ë³´
    """
    best_by_doc = {}
    best_chunk_by_doc = {}

    for score, hit in scored_hits:
        src = hit["_source"]
        docid = src.get("docid")
        if docid is None:
            continue
        if (docid not in best_by_doc) or (score > best_by_doc[docid]):
            best_by_doc[docid] = score
            best_chunk_by_doc[docid] = {
                "score": float(score),
                "content": src.get("content", ""),
                "chunk_id": src.get("chunk_id", "")
            }

    doc_sorted = sorted(best_by_doc.items(), key=lambda x: x[1], reverse=True)
    top_docids = [d for d, _ in doc_sorted[:k_doc]]
    references = [best_chunk_by_doc[d] for d in top_docids]

    return top_docids, references


# =========================
# 6) LLM í”„ë¡¬í”„íŠ¸/ë„êµ¬ ì •ì˜
# =========================

# ----- í”„ë¡¬í”„íŠ¸ ì •ì˜ -----
persona_qa = """
## Role: ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€

## Instructions
- ì‚¬ìš©ìì˜ ì´ì „ ë©”ì‹œì§€ ì •ë³´ ë° ì£¼ì–´ì§„ Reference ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
- ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ë‹µì„ í•  ìˆ˜ ì—†ë‹¤ê³  ëŒ€ë‹µí•œë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
"""

# ----- ë„êµ¬ ì •ì˜ (Function Calling) -----
persona_function_calling = """
## Role: ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€

## Instruction
- ì‚¬ìš©ìê°€ ëŒ€í™”ë¥¼ í†µí•´ ê³¼í•™ ì§€ì‹ì— ê´€í•œ ì£¼ì œë¡œ ì§ˆë¬¸í•˜ë©´ search apië¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆì–´ì•¼ í•œë‹¤.
- ê³¼í•™ ìƒì‹ê³¼ ê´€ë ¨ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ëŒ€í™” ë©”ì‹œì§€ì—ëŠ” ì ì ˆí•œ ëŒ€ë‹µì„ ìƒì„±í•œë‹¤.
"""

tools = [
    {
        "type": "function",
        "function": {
            "name": "search",
            "description": "search relevant documents",
            "parameters": {
                "properties": {
                    "standalone_query": {
                        "type": "string",
                        "description": "Final query suitable for use in search from the user messages history."
                    }
                },
                "required": ["standalone_query"],
                "type": "object"
            }
        }
    },
]


# =========================
# 7) RAG íŒŒì´í”„ë¼ì¸ (Hybrid + Rerank)
# =========================

# ----- config ê°’ ë¡œë“œ -----
retrieval_cfg = config.get("retrieval", {})

# ----- Hybrid ê²€ìƒ‰ í•¨ìˆ˜ -----
def hybrid_search_with_rerank(
        query: str,
        k_final: int = 3,
        bm25_k: int = 50,
        knn_k: int = 50) -> Tuple[List[str], List[Dict[str, Any]]]:
    bm25 = sparse_retrieve(query, size=retrieval_cfg["bm25_k"])
    knn = dense_retrieve(query, size=retrieval_cfg["knn_k"])

    bm25_hits = bm25["hits"]["hits"]
    knn_hits = knn["hits"]["hits"]

    candidates = merge_hits(bm25_hits, knn_hits, limit=retrieval_cfg["merge_limit"])
    reranked = rerank(query, candidates, topn=retrieval_cfg["num_candidates"])

    topk_docids, references = select_topk_docids(reranked, k_doc=k_final)
    return topk_docids, references


# ----- RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ -----
from typing import Optional, Callable

def answer_question(
    messages: List[Dict[str, str]],
    progress: Optional[Callable[[str], None]] = None
) -> Dict[str, Any]:
    """
    - router(ì§ˆì˜ ë¶„ì„ + tool call) â†’ retrieve(hybrid + rerank) â†’ qa(ìµœì¢… ë‹µë³€ ìƒì„±)
    - ìƒìœ„(eval_rag ë“±)ì—ì„œ tqdm.set_postfix(...)ë¥¼ ì—…ë°ì´íŠ¸í•˜ê¸° ìœ„í•œ ì½œë°±
    - ì‚¬ìš© ì˜ˆ: progress("router"), progress("retrieve"), progress("qa"), progress("done")
    """
    response = {"standalone_query": "", "topk": [], "references": [], "answer": ""}

    # 1) Router ë‹¨ê³„ (tool call ìœ ë„)
    if progress:
        progress("router")

    msg = [{"role": "system", "content": persona_function_calling}] + messages
    try:
        result = client.chat.completions.create(
            model=llm_model,
            messages=msg,
            tools=tools,
            tool_choice={"type": "function", "function": {"name": "search"}},
            temperature=0,
            seed=1,
            timeout=10
        )
    except Exception:
        traceback.print_exc()
        if progress:
            progress("error_router")
        return response

    # 2) tool callì´ ìˆìœ¼ë©´: ê²€ìƒ‰ â†’ rerank â†’ QA
    if result.choices[0].message.tool_calls:
        if progress:
            progress("parse_tool")

        tool_call = result.choices[0].message.tool_calls[0]
        function_args = json.loads(tool_call.function.arguments)
        standalone_query = function_args.get("standalone_query", "")

        response["standalone_query"] = standalone_query

        # 3) Retrieval ë‹¨ê³„ (Hybrid + Rerank)
        if progress:
            progress("retrieve")

        # Hybrid ê²€ìƒ‰ + Rerank + Top-k docid ì„ íƒ
        topk_docids, references = hybrid_search_with_rerank(
            standalone_query,
            k_final=3,
            bm25_k=50,
            knn_k=50
        )

        response["topk"] = topk_docids
        response["references"] = references

        # 4) QA ë‹¨ê³„ (ì„ íƒëœ reference contentë§Œ LLM ì»¨í…ìŠ¤íŠ¸ë¡œ ì „ë‹¬)
        if progress:
            progress("qa")

        retrieved_context = [r["content"] for r in references]
        content = json.dumps(retrieved_context, ensure_ascii=False)

        # âš ï¸ ì£¼ì˜: messagesë¥¼ in-placeë¡œ ìˆ˜ì •í•¨ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼ ë™ì‘)
        messages.append({"role": "assistant", "content": content})
        msg = [{"role": "system", "content": persona_qa}] + messages

        try:
            qaresult = client.chat.completions.create(
                model=llm_model,
                messages=msg,
                temperature=0,
                seed=1,
                timeout=30
            )
        except Exception:
            traceback.print_exc()
            if progress:
                progress("error_qa")
            return response

        response["answer"] = qaresult.choices[0].message.content

        if progress:
            progress("done")

    # 5) tool callì´ ì—†ìœ¼ë©´: router ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë‹µë³€ìœ¼ë¡œ ì‚¬ìš©
    else:
        if progress:
            progress("no_tool")

        response["answer"] = result.choices[0].message.content

        if progress:
            progress("done")

    return response

# ----- RAG í‰ê°€ í•¨ìˆ˜ -----
def eval_rag(eval_filename: str, output_filename: str):
    with open(eval_filename) as f, open(output_filename, "w") as of:
        pbar = tqdm(f, desc="Evaluating", unit="query")

        for idx, line in enumerate(pbar):
            j = json.loads(line)

            def _progress(stage: str):
                pbar.set_postfix(stage=stage, idx=idx)

            response = answer_question(j["msg"], progress=_progress)

            output = {
                "eval_id": j["eval_id"],
                "standalone_query": response["standalone_query"],
                "topk": response["topk"],
                "answer": response["answer"],
                "references": response["references"]
            }
            of.write(f"{json.dumps(output, ensure_ascii=False)}\n")

        pbar.close()


# =========================
# 8) ì‹¤í–‰ë¶€: (ì¬)ìƒ‰ì¸ + í‰ê°€
# =========================

# ----- config ê°’ ë¡œë“œ -----
paths_cfg = config.get("paths", {})
file_names_cfg = config.get("file_names", {})
chunk_cfg = config.get("chunking", {})

if __name__ == "__main__":
    
    # 1) index ìƒì„±
    create_es_index(ES_INDEX, settings, mappings)
    
    # 2) documents ë¡œë“œ
    with open(paths_cfg["raw_dir"] + "/" + file_names_cfg["documents"], "r", encoding="utf-8") as f:
        raw_docs = [json.loads(line) for line in f]

    # 3) chunking ìˆ˜í–‰
    chunked_docs = make_chunk_docs(
        raw_docs,
        chunk_size=chunk_cfg["chunk_size"],
        chunk_overlap=chunk_cfg["chunk_overlap"]
    )
    print(f"raw docs: {len(raw_docs)} -> chunked docs: {len(chunked_docs)}")

    # 4) chunk ë‹¨ìœ„ Embedding ìƒì„±
    embeddings = get_embeddings_in_batches(chunked_docs, batch_size=128)

    # 5) Elasticsearch ìƒ‰ì¸ìš© ë¬¸ì„œ êµ¬ì„±
    index_docs = []
    for doc, emb in zip(chunked_docs, embeddings):
        doc["embeddings"] = emb
        index_docs.append(doc)

    # 6) bulk indexing ì‹¤í–‰
    ret = bulk_add(ES_INDEX, index_docs)
    print(ret)

    # (sanity) ê°„ë‹¨ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_query = "ê¸ˆì„±ì´ ë‹¤ë¥¸ í–‰ì„±ë“¤ë³´ë‹¤ ë°ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    topk, refs = hybrid_search_with_rerank(test_query, k_final=3, bm25_k=20, knn_k=20)
    print("TOPK docids:", topk)
    for r in refs:
        print("rerank_score:", r["score"], "chunk_id:", r["chunk_id"])
        print("content:", r["content"][:200], "...\n")

    # 7) í‰ê°€ ì‹¤í–‰
    eval_rag(paths_cfg["raw_dir"] + "/" + file_names_cfg["eval_input"], paths_cfg["pred_dir"] + "/" + file_names_cfg["output_file"])