import os
import json
import traceback
from tqdm import tqdm
import yaml

from typing import List, Dict, Any, Tuple
from dotenv import load_dotenv
from pathlib import Path

from elasticsearch import Elasticsearch, helpers
from sentence_transformers import SentenceTransformer, CrossEncoder
import torch

from openai import OpenAI


# =========================
# í™˜ê²½/ëª¨ë¸ ì„¤ì •
# =========================
from dotenv import load_dotenv
import os

_ = load_dotenv()

# ----- OpenAI -----
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")

client = OpenAI(api_key=OPENAI_API_KEY)

# ----- Embedding / Reranker -----
EMBED_MODEL_NAME = "intfloat/multilingual-e5-base"
RERANKER_MODEL_NAME = "BAAI/bge-reranker-v2-m3"

embed_model = SentenceTransformer(EMBED_MODEL_NAME)
reranker = CrossEncoder(RERANKER_MODEL_NAME)

# ----- Elasticsearch -----
ES_USERNAME = os.getenv("ES_USERNAME")
ES_PASSWORD = os.getenv("ES_PASSWORD")
ES_INDEX = "test"

es = Elasticsearch(
    ["https://localhost:9200"],
    basic_auth=(ES_USERNAME, ES_PASSWORD),
    ca_certs="/opt/elasticsearch-8.8.0/config/certs/http_ca.crt"
)

# print("ğŸ·ï¸ Elasticsearch ì •ë³´ :", es.info())

primary_reranker = CrossEncoder("Dongjin-kr/ko-reranker", device="cuda" if torch.cuda.is_available() else "cpu")
secondary_reranker = CrossEncoder("BAAI/bge-reranker-v2-m3", device="cuda" if torch.cuda.is_available() else "cpu")

# =========================
# Config ë¡œë“œ
# =========================
CONFIG_PATH = Path("../configs/config.yaml")

with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = yaml.safe_load(f)

# print("ğŸ·ï¸ config ì •ë³´ :", config)


# =========================
# 1) Chunking
# =========================

# ----- ë¬¸ì ë‹¨ìœ„ ê¸°ë°˜ chunking í•¨ìˆ˜ -----
def chunk_text(
        text: str,
        chunk_size: int = 700,
        chunk_overlap: int = 150) -> List[str]:
    
    """
    (1) ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ chunk_size ê¸¸ì´ë§Œí¼ ìˆœì°¨ì ìœ¼ë¡œ ìë¥¸ë‹¤.
    (2) ê° chunkëŠ” ì´ì „ chunkì™€ chunk_overlap ë§Œí¼ ê²¹ì¹˜ë„ë¡ êµ¬ì„±í•œë‹¤.
       â†’ ë¬¸ë§¥ ë‹¨ì ˆì„ ì™„í™”í•˜ê³  ê²€ìƒ‰ recallì„ ë†’ì´ê¸° ìœ„í•¨
    (3) ë„ˆë¬´ ì§§ì€ chunk(30ì ë¯¸ë§Œ)ëŠ” ì •ë³´ëŸ‰ì´ ì ì–´ ì œê±°í•œë‹¤.
    (4) ì–¸ì–´ì— ì˜ì¡´í•˜ì§€ ì•ŠëŠ” ë¬¸ì ê¸°ì¤€ ë¶„í• ì´ë¯€ë¡œ ë‹¤êµ­ì–´ ë¬¸ì„œì—ì„œë„ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•œë‹¤.
    """

    if not text:
        return []

    text = text.strip()
    if len(text) <= chunk_size:
        return [text]

    chunks = []
    start = 0
    while start < len(text):
        end = min(len(text), start + chunk_size)
        chunk = text[start:end].strip()
        if len(chunk) >= 30:
            chunks.append(chunk)
        if end == len(text):
            break
        start = max(0, end - chunk_overlap)

    return chunks

# ----- ë¬¸ì„œ ë‹¨ìœ„ â†’ ì²­í‚¹ ë‹¨ìœ„ ë³€í™˜ í•¨ìˆ˜ -----
def make_chunk_docs(
        docs: List[Dict[str, Any]],
        chunk_size: int = 700,
        chunk_overlap: int = 150) -> List[Dict[str, Any]]:
    
    """
    (1) ì…ë ¥ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ì—ì„œ ê° ë¬¸ì„œì˜ docidì™€ contentë¥¼ ì½ëŠ”ë‹¤.
    (2) contentë¥¼ chunk_text()ë¡œ ë¶„í• í•˜ì—¬ ì—¬ëŸ¬ chunkë¡œ ìƒì„±í•œë‹¤.
    (3) ê° chunkëŠ” ì›ë¬¸ docidë¥¼ ìœ ì§€í•˜ë©°,
       chunk_id = "{docid}_{ìˆœë²ˆ}" í˜•íƒœë¡œ ê³ ìœ  ì‹ë³„ìë¥¼ ë¶€ì—¬í•œë‹¤.
    (4) ì´í›„ ê²€ìƒ‰, rerank, reference ì„ íƒ ë‹¨ê³„ì—ì„œ
       chunk ë‹¨ìœ„ ê²€ìƒ‰ â†’ doc ë‹¨ìœ„ ì§‘ê³„ê°€ ê°€ëŠ¥í•˜ë„ë¡ ì„¤ê³„ë˜ì—ˆë‹¤.
    """

    chunked = []
    for d in docs:
        docid = d.get("docid")
        content = d.get("content", "")
        chunks = chunk_text(
            content,
            chunk_size=chunk_size, chunk_overlap=chunk_overlap)

        for ci, c in enumerate(chunks):
            chunked.append({
                "docid": docid,                 # ì›ë¬¸ doc id ìœ ì§€
                "chunk_id": f"{docid}_{ci}",     # chunk ì‹ë³„ì
                "content": c                    # ES ê²€ìƒ‰ ëŒ€ìƒì€ chunk content
            })
    return chunked


# =========================
# 2) Multilingual Embedding
# =========================

# ----- E5 ê³„ì—´ì€ passage / query prefix -----
def e5_passage(text: str) -> str:
    return f"passage: {text}"

def e5_query(text: str) -> str:
    return f"query: {text}"

# ----- Embedding ìƒì„± í•¨ìˆ˜ -----
def get_embedding_passages(passages: List[str]) -> List[List[float]]:
    vecs = embed_model.encode(
        [e5_passage(p) for p in passages],
        normalize_embeddings=True,
        show_progress_bar=False
    )
    return vecs.tolist()

def get_embedding_query(query: str) -> List[float]:
    vec = embed_model.encode(
        [e5_query(query)],
        normalize_embeddings=True,
        show_progress_bar=False
    )[0]
    return vec.tolist()

def get_embeddings_in_batches(
        docs: List[Dict[str, Any]],
        batch_size: int = 128) -> List[List[float]]:
    all_vecs = []
    with tqdm(total=len(docs), desc="Embedding", unit="doc") as pbar:
        for i in range(0, len(docs), batch_size):
            batch = docs[i:i + batch_size]
            passages = [doc["content"] for doc in batch]
            vecs = get_embedding_passages(passages)
            all_vecs.extend(vecs)
            pbar.update(len(batch))
            pbar.set_postfix(batch=f"{i}:{i+len(batch)}")
    return all_vecs


# =========================
# 3) ES ì¸ë±ìŠ¤/ìƒ‰ì¸
# =========================

# ----- í•œêµ­ì–´ ë§ì¶¤í˜• ë¶„ì„ê¸° ì„¤ì • -----
settings = {
    "analysis": {
        "analyzer": {
            "nori": {
                "type": "custom",
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",         # ë³µí•©ì–´ë¥¼ ë¶„í•´í•˜ë©´ì„œ ì›í˜•ë„ í•¨ê»˜ ìœ ì§€ (ê²€ìƒ‰ recall â†‘)
                "filter": ["nori_posfilter"]        # í•œêµ­ì–´ í’ˆì‚¬ ê¸°ë°˜ stop filter
            }
        },
        "filter": {
            # ì¡°ì‚¬(J), ì–´ë¯¸(E), êµ¬ë‘ì (S*) ë“± ì˜ë¯¸ ê¸°ì—¬ë„ê°€ ë‚®ì€ í’ˆì‚¬ ì œê±°
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": ["E", "J", "SC", "SE", "SF", "VCN", "VCP", "VX"]
            }
        }
    }
}

# ----- ì¸ë±ìŠ¤ ë§¤í•‘ ì •ì˜ -----
mappings = {
    "properties": {
        "docid": {"type": "keyword"},
        "chunk_id": {"type": "keyword"},
        "content": {"type": "text", "analyzer": "nori"},
        "embeddings": {
            "type": "dense_vector",
            "dims": 768,
            "index": True,
            "similarity": "cosine"
        }
    }
}

# ----- ì¸ë±ìŠ¤ ìƒì„± í•¨ìˆ˜ -----
def create_es_index(
        index: str,
        settings: Dict[str, Any],
        mappings: Dict[str, Any]) -> None:
    """
    (1) ë™ì¼í•œ ì´ë¦„ì˜ ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ë©´ ì‚­ì œ
    (2) settings + mappingsë¥¼ ì ìš©í•˜ì—¬ ì¸ë±ìŠ¤ë¥¼ ì¬ìƒì„±
    â†’ ì‹¤í—˜ ë°˜ë³µ ì‹œ í•­ìƒ ë™ì¼í•œ ìƒíƒœì—ì„œ ì‹œì‘í•˜ê¸° ìœ„í•¨
    """
    if es.indices.exists(index=index):
        es.indices.delete(index=index)
    es.indices.create(index=index, settings=settings, mappings=mappings)

# ----- Bulk ìƒ‰ì¸ í•¨ìˆ˜ -----
def bulk_add(
        index: str, 
        docs: List[Dict[str, Any]]) -> Any:
    """
    (1) helpers.bulkë¥¼ ì‚¬ìš©í•´ ëŒ€ëŸ‰ ë¬¸ì„œë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ ìƒ‰ì¸
    (2) ê° ë¬¸ì„œëŠ” chunk ë‹¨ìœ„ì´ë©°, content(BM25) + embeddings(Dense)ê°€ í•¨ê»˜ ì €ì¥ë¨
    """
    actions = [{"_index": index, "_source": doc} for doc in docs]
    return helpers.bulk(es, actions)


# =========================
# 4) ê²€ìƒ‰ (BM25 + KNN í›„ë³´ ìƒì„±)
# =========================

# ----- Sparse Retrieval (BM25) -----
def sparse_retrieve(query_str: str, size: int = 50):
    query = {
        "match": {
            "content": {
                "query": query_str
            }
        }
    }
    return es.search(index=ES_INDEX, query=query, size=size, sort="_score")

# ----- Dense Retrieval (KNN) -----
def dense_retrieve(query_str: str, size: int = 50, num_candidates: int = 200):
    query_vec = get_embedding_query(query_str)
    knn = {
        "field": "embeddings",
        "query_vector": query_vec,
        "k": size,
        "num_candidates": num_candidates
    }
    return es.search(index=ES_INDEX, knn=knn)

# ----- BM25 + KNN ê²°ê³¼ ë³‘í•© í•¨ìˆ˜ (chunk_id ê¸°ì¤€ ì¤‘ë³µ ì—†ì´ í•©ì¹˜ê¸°)-----
def merge_hits(bm25_hits, knn_hits, limit: int = 100) -> List[Dict[str, Any]]:
    merged = {}
    for h in bm25_hits:
        cid = h["_source"].get("chunk_id")
        if cid and cid not in merged:
            merged[cid] = h
    for h in knn_hits:
        cid = h["_source"].get("chunk_id")
        if cid and cid not in merged:
            merged[cid] = h

    # ì´ˆê¸° í›„ë³´ëŠ” ES score ê¸°ë°˜ìœ¼ë¡œ ëŒ€ì¶© ì •ë ¬í•´ì„œ limitë¡œ ìë¦„
    cand = list(merged.values())
    cand.sort(key=lambda x: (x.get("_score", 0.0)), reverse=True)
    return cand[:limit]


# =========================
# 5) Reranker (CrossEncoder)
# =========================

# ----- CrossEncoder ê¸°ë°˜ Rerank í•¨ìˆ˜ -----
def rerank(
        query: str, hits: List[Dict[str, Any]],
        topn: int = 20) -> List[Tuple[float, Dict[str, Any]]]:
    """
    (1) (query, chunk_content) ìŒì„ í›„ë³´ ê°œìˆ˜ë§Œí¼ ìƒì„±
       â†’ CrossEncoderëŠ” queryì™€ passageë¥¼ "í•¨ê»˜" ì…ë ¥ìœ¼ë¡œ ë°›ì•„ ìƒí˜¸ì‘ìš© ê¸°ë°˜ ì ìˆ˜ë¥¼ ê³„ì‚°
    (2) reranker.predict(pairs)ë¡œ ê° pairì˜ relevance score ì‚°ì¶œ
       â†’ ì ìˆ˜ ìŠ¤ì¼€ì¼ì€ ëª¨ë¸ ë‚´ë¶€ ê¸°ì¤€ì´ë©°, ES _scoreì™€ ì§ì ‘ ë¹„êµí•˜ì§€ ì•ŠìŒ
    (3) score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìƒìœ„ topnë§Œ ë°˜í™˜
       â†’ ì´í›„ ë‹¨ê³„ì—ì„œ docid ë‹¨ìœ„ ì§‘ê³„(select_topk_docids)ì— ì‚¬ìš©
    (4) ë°˜í™˜:
    - [(rerank_score, hit), ...] í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ (ìƒìœ„ topn)
    - hitì—ëŠ” ì›ë˜ ES hit dictê°€ ê·¸ëŒ€ë¡œ í¬í•¨ë¨ (_source í™œìš© ê°€ëŠ¥)
    """
    pairs = [(query, h["_source"]["content"]) for h in hits]
    if not pairs:
        return []

    scores = reranker.predict(pairs)  # numpy array
    scored = list(zip(scores.tolist(), hits))
    scored.sort(key=lambda x: x[0], reverse=True)
    return scored[:topn]


def select_topk_docids(
        scored_hits: List[Tuple[float, Dict[str, Any]]],
        k_doc: int = 3) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    (1) scored_hitsë¥¼ ìˆœíšŒí•˜ë©° docidë³„ ìµœê³  rerank scoreë¥¼ ê¸°ë¡(best_by_doc)
       â†’ "docì˜ ëŒ€í‘œ ì ìˆ˜ = í•´ë‹¹ docì—ì„œ ê°€ì¥ ê´€ë ¨ ë†’ì€ chunkì˜ ì ìˆ˜"
    (2) docidë³„ë¡œ ìµœê³  ì ìˆ˜ë¥¼ ë§Œë“  chunkë¥¼ referencesë¡œ ì €ì¥(best_chunk_by_doc)
       â†’ ì´í›„ LLMì— ë„˜ê¸¸ reference contextë¡œ ì‚¬ìš©ë¨
    (3) docid ëŒ€í‘œ ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„ ìƒìœ„ k_docê°œ docid ì„ íƒ
    (4) ë°˜í™˜:
       - top_docids: ì„ íƒëœ docid ë¦¬ìŠ¤íŠ¸
       - references: ê° docidì—ì„œ ìµœê³  ì ìˆ˜ chunkì˜ {score, content, chunk_id} ì •ë³´
    """
    best_by_doc = {}
    best_chunk_by_doc = {}

    for score, hit in scored_hits:
        src = hit["_source"]
        docid = src.get("docid")
        if docid is None:
            continue
        if (docid not in best_by_doc) or (score > best_by_doc[docid]):
            best_by_doc[docid] = score
            best_chunk_by_doc[docid] = {
                "score": float(score),
                "content": src.get("content", ""),
                "chunk_id": src.get("chunk_id", "")
            }

    doc_sorted = sorted(best_by_doc.items(), key=lambda x: x[1], reverse=True)
    top_docids = [d for d, _ in doc_sorted[:k_doc]]
    references = [best_chunk_by_doc[d] for d in top_docids]

    return top_docids, references


# =========================
# 6) LLM í”„ë¡¬í”„íŠ¸/ë„êµ¬ ì •ì˜
# =========================

# ----- í”„ë¡¬í”„íŠ¸ ì •ì˜ -----

persona_router = """
## Role: ì§€ì‹ ê²€ìƒ‰ ë¼ìš°í„° (Retrieval Router)

## í•µì‹¬ ëª©í‘œ
- ì‚¬ìš©ìì˜ ë°œí™”ê°€ "ë¬¸ì„œ DB ê²€ìƒ‰ìœ¼ë¡œ ë‹µí•  ìˆ˜ ìˆëŠ” ì§€ì‹ ì§ˆë¬¸"ì´ë©´ needs_search=trueë¡œ ì„¤ì •í•˜ê³ ,
  ê²€ìƒ‰ì— ìµœì í™”ëœ standalone_queryë¥¼ ìƒì„±í•œë‹¤.
- ì¡ë‹´/ì¸ì‚¬/ê°ì‚¬/ë©”íƒ€ ì§ˆë¬¸/ì‹œìŠ¤í…œ ì§ˆë¬¸(ì˜ˆ: ì½”ë“œ ì—ëŸ¬, í™˜ê²½ ì„¤ì •)ì€ needs_search=falseë¡œ ì„¤ì •í•˜ê³ ,
  brief_replyì— ì§§ê²Œ ë‹µí•œë‹¤. ì´ ê²½ìš° standalone_queryëŠ” ë¹ˆ ë¬¸ìì—´.

## íŒë‹¨ ê¸°ì¤€ (ê°€ì¥ ì¤‘ìš”)
- ì§€ì‹ ì§ˆë¬¸(ì‚¬ì‹¤/ê°œë…/ì„¤ëª…/ì›ë¦¬/ì •ì˜/ì—­ì‚¬/ì¸ë¬¼/ë¬¸í™”/ì‚¬íšŒ/ê¸°ìˆ  ë“±) => needs_search=true
- ì¡ë‹´/ì¸ì‚¬/ê°ì‚¬/ëŒ€í™” ìœ ì§€/ë©”íƒ€ ì§ˆë¬¸(â€œë„ˆ ëˆ„êµ¬ì•¼?â€, â€œë°©ê¸ˆ ë­ë¼ê³  í–ˆì–´?â€)/ì½”ë“œ ë””ë²„ê¹… => needs_search=false

## ê²€ìƒ‰ì–´(standalone_query) ìƒì„± ì›ì¹™ (needs_search=trueì¼ ë•Œë§Œ)
1) í•œêµ­ì–´ ìœ„ì£¼ + í•µì‹¬ í‚¤ì›Œë“œ ë‚˜ì—´ (ë¬¸ì¥ ê¸ˆì§€)
2) ì˜ì–´ ê³ ìœ ëª…ì‚¬/ì „ë¬¸ìš©ì–´/ì•½ì–´ í¬í•¨ ì‹œ: í•œê¸€ ë²ˆì—­ì–´ + ì›ì–´(ì•½ì–´) í•¨ê»˜ í¬í•¨
3) ëŒ€ëª…ì‚¬(ê·¸ê²ƒ/ê·¸ê±°/ì´ê²ƒ ë“±)ë‚˜ ìƒëµëœ ì£¼ì–´ëŠ” ëŒ€í™” ë§¥ë½ìœ¼ë¡œ êµ¬ì²´í™”
4) ìˆ«ì/ë‹¨ìœ„/ê¸°í˜¸/í™”í•™ì‹/ì•½ì–´ëŠ” ì‚­ì œí•˜ì§€ ë§ ê²ƒ
5) í‚¤ì›Œë“œëŠ” 4~10ê°œ ì •ë„ë¡œ ê°„ê²°í•˜ê²Œ, ì¤‘ë³µ ì œê±°

## ì¶œë ¥ í˜•ì‹
- ë°˜ë“œì‹œ ì•„ë˜ JSON í•œ ì¤„ë§Œ ì¶œë ¥ (ë‹¤ë¥¸ í…ìŠ¤íŠ¸ ê¸ˆì§€)
{"needs_search": true/false, "standalone_query": "...", "brief_reply": "..."}

- needs_search=falseì´ë©´:
  - standalone_queryëŠ” "" (ë¹ˆ ë¬¸ìì—´)
  - brief_replyëŠ” ì§§ê³  ì •ì¤‘í•˜ê²Œ

- needs_search=trueì´ë©´:
  - brief_replyëŠ” "" (ë¹ˆ ë¬¸ìì—´)
"""

persona_qa = """
## Role: ê³¼í•™ ìƒì‹ ì „ë¬¸ê°€

## Instructions
- ì‚¬ìš©ìì˜ ì´ì „ ë©”ì‹œì§€ ì •ë³´ ë° ì£¼ì–´ì§„ Reference ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ê°„ê²°í•˜ê²Œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
- ì£¼ì–´ì§„ ê²€ìƒ‰ ê²°ê³¼ ì •ë³´ë¡œ ëŒ€ë‹µí•  ìˆ˜ ì—†ëŠ” ê²½ìš°ëŠ” ì •ë³´ê°€ ë¶€ì¡±í•´ì„œ ë‹µì„ í•  ìˆ˜ ì—†ë‹¤ê³  ëŒ€ë‹µí•œë‹¤.
- í•œêµ­ì–´ë¡œ ë‹µë³€ì„ ìƒì„±í•œë‹¤.
"""


tools = [
    {
        "type": "function",
        "function": {
            "name": "route",
            "description": "Decide whether to search documents and generate a standalone query (if needed).",
            "parameters": {
                "type": "object",
                "properties": {
                    "needs_search": {
                        "type": "boolean",
                        "description": "If true, run retrieval. If false, do NOT retrieve and answer briefly."
                    },
                    "standalone_query": {
                        "type": "string",
                        "description": "Search query in Korean keywords. Only meaningful when needs_search=true."
                    },
                    "brief_reply": {
                        "type": "string",
                        "description": "Brief reply when needs_search=false."
                    },
                },
                "required": ["needs_search", "standalone_query", "brief_reply"]
            }
        }
    }
]


# =========================
# 7) RAG íŒŒì´í”„ë¼ì¸ (Hybrid + Rerank)
# =========================

# ----- config ê°’ ë¡œë“œ -----
retrieval_cfg = config.get("retrieval", {})

import numpy as np
from sentence_transformers import CrossEncoder

def dual_stage_rerank(
    query: str,
    candidates: list[dict],
    primary_model: CrossEncoder,
    secondary_model: CrossEncoder,
    stage1_k: int = 150,
    stage2_k: int = 50,
    w1: float = 0.6,
    w2: float = 0.4,
) -> list[dict]:
    """
    2-Stage rerank + z-score fusion
    candidates: [{"docid":..., "content":..., "meta":..., "score":...}, ...] í˜•íƒœë¥¼ ê°€ì •
    ë°˜í™˜: candidatesì™€ ê°™ì€ dict ë¦¬ìŠ¤íŠ¸ + "rerank_score" í•„ë“œ ì¶”ê°€(ì •ë ¬ë¨)
    """
    if not candidates:
        return []

    # ---------- Stage 1 (fast reranker) ----------
    pairs1 = [[query, c["content"]] for c in candidates]
    s1 = np.array(primary_model.predict(pairs1, batch_size=32, show_progress_bar=False), dtype=np.float32)

    # ìƒìœ„ stage1_kë¡œ ì»·
    stage1_k = min(stage1_k, len(candidates))
    idx1 = np.argsort(s1)[::-1][:stage1_k]
    cand1 = [candidates[i] for i in idx1]
    s1_cut = s1[idx1]

    # ---------- Stage 2 (strong reranker) ----------
    pairs2 = [[query, c["content"]] for c in cand1]
    s2 = np.array(secondary_model.predict(pairs2, batch_size=16, show_progress_bar=False), dtype=np.float32)

    # ---------- z-score normalize ----------
    s1n = (s1_cut - s1_cut.mean()) / (s1_cut.std() + 1e-8)
    s2n = (s2     - s2.mean())     / (s2.std()     + 1e-8)

    # ---------- fusion ----------
    final = w1 * s1n + w2 * s2n

    # ---------- final sort ----------
    stage2_k = min(stage2_k, len(cand1))
    idx2 = np.argsort(final)[::-1][:stage2_k]
    out = []
    for i in idx2:
        item = cand1[i].copy()
        item["rerank_score"] = float(final[i])
        out.append(item)

    return out

# ----- Hybrid ê²€ìƒ‰ í•¨ìˆ˜ -----
from typing import List, Dict, Any, Tuple

def hybrid_search_with_rerank(
    query: str,
    k_final: int = 3,
    bm25_k: int = 50,
    knn_k: int = 50,
    merge_limit: int = 200,
) -> Tuple[List[str], List[Dict[str, Any]]]:
    """
    1) BM25 / KNN retrieve
    2) merge + dedup (docid ê¸°ì¤€) + merge score
    3) 2-stage rerank (dual_stage_rerank)
    4) ìµœì¢… topk ë°˜í™˜
    """

    # -------------------------
    # 1) Retrieve
    # -------------------------
    bm25 = sparse_retrieve(query, size=bm25_k)
    knn  = dense_retrieve(query, size=knn_k)

    bm25_hits = bm25.get("hits", {}).get("hits", [])
    knn_hits  = knn.get("hits", {}).get("hits", [])

    # -------------------------
    # 2) Merge + Dedup
    # -------------------------
    merged_dict: Dict[str, Dict[str, Any]] = {}

    # âœ… ë„ˆ í”„ë¡œì íŠ¸ì— ì´ë¯¸ ìˆìœ¼ë©´ ì´ 2ê°œë¥¼ ì“°ëŠ” ê²Œ ì œì¼ ì•ˆì „í•¨
    # ES_ID_FIELD = config/envì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë¬¸ì„œ ID í•„ë“œëª…
    # ES_TEXT_FIELD = config/envì—ì„œ ê°€ì ¸ì˜¤ëŠ” ë³¸ë¬¸ í•„ë“œëª…
    # ì—†ìœ¼ë©´ ì•„ë˜ fallback ë¡œì§ì´ ë™ì‘í•¨.

    ES_TEXT_FIELD = "content"
    ES_ID_FIELD = "doc_id"

    def _hit_to_doc(h: Dict[str, Any]) -> Tuple[str, str, Dict[str, Any]]:
        src = h.get("_source", {}) or {}

        # docid ìš°ì„ ìˆœìœ„: _source[ES_ID_FIELD] > _source["docid"] > _id
        docid = None
        if "ES_ID_FIELD" in globals():
            docid = src.get(ES_ID_FIELD)
        if not docid:
            docid = src.get("docid")
        if not docid:
            docid = h.get("_id")

        # content ìš°ì„ ìˆœìœ„: _source[ES_TEXT_FIELD] > _source["content"] > _source["text"]
        content = ""
        if "ES_TEXT_FIELD" in globals():
            content = src.get(ES_TEXT_FIELD, "") or ""
        if not content:
            content = src.get("content") or src.get("text") or ""

        return docid, content, src

    # (1) BM25 ë„£ê¸°
    for h in bm25_hits:
        docid, content, meta = _hit_to_doc(h)
        if not docid or not content:
            continue
        merged_dict[docid] = {
            "docid": docid,
            "content": content,
            "meta": meta,
            "bm25_score": float(h.get("_score", 0.0)),
            "knn_score": 0.0,
        }

    # (2) KNN ë„£ê¸° (dedup + score í•©ì¹˜ê¸°)
    for h in knn_hits:
        docid, content, meta = _hit_to_doc(h)
        if not docid or not content:
            continue

        if docid not in merged_dict:
            merged_dict[docid] = {
                "docid": docid,
                "content": content,
                "meta": meta,
                "bm25_score": 0.0,
                "knn_score": float(h.get("_score", 0.0)),
            }
        else:
            merged_dict[docid]["knn_score"] = float(h.get("_score", 0.0))

    # (3) merge score (ê°„ë‹¨ í•©ì‚° ë²„ì „)
    # í•„ìš”í•˜ë©´ ì—¬ê¸°ì„œ RRFë¡œ ë°”ê¿€ ìˆ˜ë„ ìˆìŒ (ì í”„ì—… í›„ë³´)
    for v in merged_dict.values():
        v["score"] = v.get("bm25_score", 0.0) + v.get("knn_score", 0.0)

    merged_hits = sorted(merged_dict.values(), key=lambda x: x["score"], reverse=True)[:merge_limit]

    # -------------------------
    # 3) Candidates ë§Œë“¤ê¸°
    # -------------------------
    candidates = [
        {
            "docid": h["docid"],
            "content": h["content"],          # âœ… reranker ì…ë ¥ í…ìŠ¤íŠ¸
            "meta": h.get("meta", {}),
            "score": float(h.get("score", 0.0)),
        }
        for h in merged_hits
    ]

    # merged ê²°ê³¼ê°€ ë¹„ë©´ ë°”ë¡œ ì¢…ë£Œ
    if not candidates:
        return [], []

    # -------------------------
    # 4) 2-stage rerank
    # -------------------------
    reranked = dual_stage_rerank(
        query=query,
        candidates=candidates,
        primary_model=primary_reranker,
        secondary_model=secondary_reranker,
        stage1_k=min(150, len(candidates)),
        stage2_k=min(50,  len(candidates)),
        w1=0.6,
        w2=0.4,
    )

    reranked = reranked[:k_final]

    topk_docids = [r["docid"] for r in reranked]
    references = [
        {
            "score": float(r.get("rerank_score", r.get("score", 0.0))),
            "content": r["content"],
            "chunk_id": f'{r["docid"]}_0'
        }
        for r in reranked
    ]

    return topk_docids, references


# ----- RAG ì „ì²´ íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ -----

from typing import Optional, Callable

llm_cfg = config.get("llm", {})
router_model = llm_cfg["router_model"]
qa_model = llm_cfg["qa_model"]

def answer_question(
    messages: List[Dict[str, str]],
    progress: Optional[Callable[[str], None]] = None
) -> Dict[str, Any]:
    response = {
        "needs_search": None,
        "standalone_query": "",
        "topk": [],
        "references": [],
        "answer": ""
    }

    if progress:
        progress("router")

    msg = [{"role": "system", "content": persona_router}] + messages

    try:
        result = client.chat.completions.create(
            model=router_model,
            messages=msg,
            tools=tools,
            tool_choice={"type": "function", "function": {"name": "route"}},  # route ê°•ì œ
            temperature=0,
            seed=1,
            timeout=10
        )
    except Exception:
        traceback.print_exc()
        if progress:
            progress("error_router")
        return response

    if progress:
        progress("parse_tool")

    # âœ… route ê°•ì œë‹ˆê¹Œ ë³´í†µ tool_callsê°€ ìˆì–´ì•¼ í•¨. ê·¸ë˜ë„ ë°©ì–´.
    tool_calls = getattr(result.choices[0].message, "tool_calls", None)
    if not tool_calls:
        response["needs_search"] = None
        response["answer"] = result.choices[0].message.content or ""
        if progress:
            progress("done")
        return response

    tool_call = tool_calls[0]
    function_args = json.loads(tool_call.function.arguments)

    needs_search = bool(function_args.get("needs_search", True))
    standalone_query = (function_args.get("standalone_query") or "").strip()
    brief_reply = (function_args.get("brief_reply") or "").strip()

    # âœ… ì—¬ê¸°ì„œ ì €ì¥í•´ì•¼ null ì•ˆ ëœ¸
    response["needs_search"] = needs_search
    response["standalone_query"] = standalone_query

    if not needs_search:
        if progress:
            progress("skip_search")
        response["topk"] = []
        response["references"] = []
        response["answer"] = brief_reply if brief_reply else "ê´œì°®ì•„. ë¬´ìŠ¨ ì¼ ìˆì—ˆì–´?"
        if progress:
            progress("done")
        return response

    if not standalone_query:
        # fallback: ë§ˆì§€ë§‰ user
        last_user = ""
        for m in reversed(messages):
            if m.get("role") == "user":
                last_user = m.get("content", "")
                break
        standalone_query = last_user.strip()
        response["standalone_query"] = standalone_query

    if progress:
        progress("retrieve")

    topk_docids, references = hybrid_search_with_rerank(
        standalone_query,
        k_final=3,
        bm25_k=50,
        knn_k=50
    )

    response["topk"] = topk_docids
    response["references"] = references

    if progress:
        progress("qa")

    retrieved_context = [r["content"] for r in references]
    content = json.dumps(retrieved_context, ensure_ascii=False)

    # âš ï¸ messages in-place ìˆ˜ì • ì‹«ìœ¼ë©´ copy ì‚¬ìš© ê¶Œì¥(í•˜ì§€ë§Œ ì§€ê¸ˆì€ ìœ ì§€)
    messages.append({"role": "assistant", "content": content})
    qa_msg = [{"role": "system", "content": persona_qa}] + messages

    try:
        qaresult = client.chat.completions.create(
            model=qa_model,
            messages=qa_msg,
            temperature=0,
            seed=1,
            timeout=30
        )
    except Exception:
        traceback.print_exc()
        if progress:
            progress("error_qa")
        return response

    response["answer"] = qaresult.choices[0].message.content or ""
    if progress:
        progress("done")
    return response


# ----- RAG í‰ê°€ í•¨ìˆ˜ -----
def eval_rag(eval_filename: str, output_filename: str):
    with open(eval_filename) as f, open(output_filename, "w") as of:
        pbar = tqdm(f, desc="Evaluating", unit="query")

        for idx, line in enumerate(pbar):
            j = json.loads(line)

            def _progress(stage: str):
                pbar.set_postfix(stage=stage, idx=idx)

            response = answer_question(j["msg"], progress=_progress)

            output = {
                "eval_id": j["eval_id"],
                "standalone_query": response["standalone_query"],
                "topk": response["topk"],
                "answer": response["answer"],
                "references": response["references"]
            }
            of.write(f"{json.dumps(output, ensure_ascii=False)}\n")

        pbar.close()


# ===== local_eval ê´€ë ¨ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ =====

def build_candidates_docid_level_for_judge(question: str, cfg: Dict[str, Any]) -> List[Dict[str, Any]]:
    c_cfg = cfg["local_eval"]["candidates"]

    bm25 = sparse_retrieve(question, size=c_cfg["sparse_k"])
    knn = dense_retrieve(
        question,
        size=c_cfg["dense_k"],
        num_candidates=c_cfg["num_candidates"]
    )

    candidates = merge_hits(
        bm25["hits"]["hits"],
        knn["hits"]["hits"],
        limit=c_cfg["merge_limit"]
    )

    reranked = rerank(question, candidates, topn=c_cfg["rerank_topn"])

    top_docids, references = select_topk_docids(reranked, k_doc=c_cfg["max_candidate_docids"])

    # local_eval judgeëŠ” [{"docid":..., "content":...}] í˜•íƒœë¥¼ ì›í•¨
    doc_candidates = []
    for docid, ref in zip(top_docids, references):
        doc_candidates.append({
            "docid": docid,
            "content": ref.get("content", "")
        })
    return doc_candidates


# =========================
# 8) ì‹¤í–‰ë¶€: (ì¬)ìƒ‰ì¸ + í‰ê°€
# =========================

# ===== config ê°’ ë¡œë“œ =====
paths_cfg = config.get("paths", {})
file_names_cfg = config.get("file_names", {})
chunk_cfg = config.get("chunking", {})

if __name__ == "__main__":
    
    # 1) index ìƒì„±
    create_es_index(ES_INDEX, settings, mappings)
    
    # 2) documents ë¡œë“œ
    with open(paths_cfg["raw_dir"] + "/" + file_names_cfg["documents"], "r", encoding="utf-8") as f:
        raw_docs = [json.loads(line) for line in f]

    # 3) chunking ìˆ˜í–‰
    chunked_docs = make_chunk_docs(
        raw_docs,
        chunk_size=chunk_cfg["chunk_size"],
        chunk_overlap=chunk_cfg["chunk_overlap"]
    )
    print(f"raw docs: {len(raw_docs)} -> chunked docs: {len(chunked_docs)}")

    # 4) chunk ë‹¨ìœ„ Embedding ìƒì„±
    embeddings = get_embeddings_in_batches(chunked_docs, batch_size=128)

    # 5) Elasticsearch ìƒ‰ì¸ìš© ë¬¸ì„œ êµ¬ì„±
    index_docs = []
    for doc, emb in zip(chunked_docs, embeddings):
        doc["embeddings"] = emb
        index_docs.append(doc)

    # 6) bulk indexing ì‹¤í–‰
    ret = bulk_add(ES_INDEX, index_docs)
    print(ret)

    # (sanity) ê°„ë‹¨ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    test_query = "ê¸ˆì„±ì´ ë‹¤ë¥¸ í–‰ì„±ë“¤ë³´ë‹¤ ë°ê²Œ ë³´ì´ëŠ” ì´ìœ ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    topk, refs = hybrid_search_with_rerank(test_query, k_final=3, bm25_k=20, knn_k=20)
    print("TOPK docids:", topk)
    for r in refs:
        print("rerank_score:", r["score"], "chunk_id:", r["chunk_id"])
        print("content:", r["content"][:200], "...\n")

    # 7) í‰ê°€ ì‹¤í–‰
    # ----- Eval RAG ì‹¤í–‰ ì—¬ë¶€ -----
    eval_rag_cfg = config.get("eval_rag", {})

    if eval_rag_cfg["enable"]:
        print("[EvalRAG] Running full eval_rag pipeline")

        eval_rag(
            paths_cfg["raw_dir"] + "/" + file_names_cfg["eval_input"],
            paths_cfg["pred_dir"] + "/" + file_names_cfg["output_file"]
        )

    else:
        print("[EvalRAG] Skipped full eval_rag (eval_rag.enable=false)")
    
    # # 8) local_evalìš© í›„ë³´ ë¬¸ì„œ ìƒì„± í…ŒìŠ¤íŠ¸
    # from local_eval import run_local_judge_eval

    # le_cfg = config.get("local_eval", {})

    # if le_cfg.get("enable", False):
    #     judge_model = le_cfg["judge"]["model"]

    #     os.makedirs(os.path.join(paths_cfg["pred_dir"], "judge_cache"), exist_ok=True)
    #     os.makedirs(os.path.join(paths_cfg["pred_dir"], "pred_cache"), exist_ok=True)

    #     judge_cache_path = os.path.join(
    #         paths_cfg["pred_dir"],
    #         "judge_cache",
    #         f"judge_cache_{judge_model}.jsonl"
    #     )

    #     llm_cfg = config.get("llm", {})
    #     router_model = llm_cfg["router_model"]
    #     safe_router = router_model.replace("/", "_")
    #     qa_model = llm_cfg["qa_model"]
    #     safe_qa = qa_model.replace("/", "_")

    #     pred_cache_path = os.path.join(
    #         paths_cfg["pred_dir"],
    #         "pred_cache",
    #         f"pred_cache_router={safe_router}_qa={safe_qa}.jsonl"
    #     )

    #     eval_path = os.path.join(paths_cfg["raw_dir"], file_names_cfg["eval_input"])

    #     # predict_fn: ê¸°ì¡´ answer_question ê·¸ëŒ€ë¡œ ì‚¬ìš© (ì§„í–‰ í‘œì‹œ ë²„ì „ì´ë©´ ê·¸ê±¸ ì¨ë„ ë¨)
    #     def predict_fn(msgs):
    #         return answer_question([m.copy() for m in msgs])

    #     # build_candidates_fn: rag ë‚´ë¶€ í•¨ìˆ˜ë¡œ ì£¼ì…
    #     def build_candidates_fn(question: str):
    #         return build_candidates_docid_level_for_judge(question, config)

    #     rows, map_score, mrr_score = run_local_judge_eval(
    #         eval_path=eval_path,
    #         judge_cache_path=judge_cache_path,
    #         pred_cache_path=pred_cache_path,
    #         judge_model=judge_model,
    #         client=client,  # ragì—ì„œ ë§Œë“  OpenAI()

    #         predict_fn=predict_fn,
    #         build_candidates_fn=build_candidates_fn,

    #         max_n=le_cfg.get("max_n", 200),
    #         k_eval=le_cfg.get("k_eval", 3),

    #         max_docs_per_question=le_cfg["judge"]["max_docs_per_question"],
    #         judge_temperature=le_cfg["judge"]["temperature"],
    #         judge_timeout=le_cfg["judge"]["timeout"],
    #         content_truncate=le_cfg["judge"]["content_truncate"],
    #     )
        
    #     print(f"[Local Eval] MAP@{le_cfg.get('k_eval', 3)}={map_score:.4f} | MRR@{le_cfg.get('k_eval', 3)}={mrr_score:.4f} | n={len(rows)}")
    #     print(f"[Local Eval] judge_cache: {judge_cache_path}")``