"""


"""

print("[START] RAG v4.9.2_jab3 (Stable Baseline) ì‹œì‘...", flush=True)

import os
import re
import json
import csv
import time
import argparse
import traceback
import hashlib
import numpy as np
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass

print("[IMPORT] ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì™„ë£Œ", flush=True)

from elasticsearch import Elasticsearch, helpers
print("[IMPORT] Elasticsearch ì™„ë£Œ", flush=True)

from sentence_transformers import SentenceTransformer, CrossEncoder
print("[IMPORT] SentenceTransformers ì™„ë£Œ", flush=True)

import torch

import logging
from datetime import datetime
from pathlib import Path

def setup_logger(log_path: str | None = None) -> logging.Logger:
    logger = logging.getLogger("rag_v4_9_2_2")
    if logger.handlers:
        return logger
    logger.setLevel(logging.INFO)
    fmt = logging.Formatter("[%(asctime)s] %(levelname)s - %(message)s")
    sh = logging.StreamHandler()
    sh.setFormatter(fmt)
    logger.addHandler(sh)
    if log_path:
        Path(log_path).parent.mkdir(parents=True, exist_ok=True)
        fh = logging.FileHandler(log_path, encoding="utf-8")
        fh.setFormatter(fmt)
        logger.addHandler(fh)
    return logger

print(f"[INFO] PyTorch ë²„ì „: {torch.__version__}", flush=True)
print(f"[INFO] CUDA ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.is_available()}", flush=True)
if torch.cuda.is_available():
    print(f"[INFO] GPU ë””ë°”ì´ìŠ¤: {torch.cuda.get_device_name(0)}", flush=True)
    print(f"[INFO] GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB", flush=True)

from openai import OpenAI
print("[IMPORT] OpenAI ì™„ë£Œ", flush=True)


# ============================================================================
# ì„¤ì • ìƒìˆ˜
# ============================================================================

# OpenAI API ì„¤ì •
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY", "your_openai_api_key_here")
openai_client = OpenAI(api_key=OPENAI_API_KEY)

# ì„ë² ë”© ëª¨ë¸ ì„¤ì • (KURE)
EMBEDDING_MODEL_NAME = "nlpai-lab/KURE-v1"
EMBEDDING_DIMS = 1024

# LLM ëª¨ë¸
QUERY_LLM_MODEL = "gpt-4o-mini"

# ê°€ì¤‘ì¹˜ ì„¤ì •
DENSE_WEIGHT = 0.7
SPARSE_WEIGHT = 0.3
FUSION_RATIO = 0.525
RERANKER_RATIO = 0.475

# Reranker ì•™ìƒë¸” ê°€ì¤‘ì¹˜
PRIMARY_RERANKER_WEIGHT = 0.65
SECONDARY_RERANKER_WEIGHT = 0.35

# ê²€ìƒ‰ ì„¤ì • (v4.9.2.2_f.l: í›„ë³´êµ° í™•ëŒ€)
CANDIDATES = 1000  # 500 â†’ 1000
FIRST_STAGE_K = 100 # 50 â†’ 100
SECOND_STAGE_K = 30 # 15 â†’ 30
FINAL_TOP_K = 3
SCORE_THRESHOLD = 0.5  # Fallback ì§„ë‹¨ìš© ì„ê³„ì¹˜ (ì •ê·œí™” ì „ ì ìˆ˜ ê¸°ì¤€)

# RRF ì„¤ì •
RRF_K = 60

# BM25 ì„¤ì •
BM25_K1 = 1.5
BM25_B = 0.75

# ë°°ì¹˜ ì„¤ì •
BATCH_SIZE = 100

# v4.9 ì„¤ì • (Rule ê¸°ë°˜)
USE_RULE_BASED = True  # Rule ê¸°ë°˜ ìš°ì„  (Fallback: LLM)
USE_SEARCH_OFF = True  # SEARCH_OFF ê¸°ëŠ¥ í™œì„±í™”

# ìºì‹œ íŒŒì¼ ê²½ë¡œ
STANDALONE_CACHE_FILE = Path("standalone_query_cache_jab6.json")
AUGMENTATION_CACHE_FILE = Path("augmentation_cache_jab6.json")
ANSWER_CACHE_FILE = Path("answer_cache_jab6.json")

class PersistentCache:
    def __init__(self, cache_file: Path):
        self.cache_file = cache_file
        self.cache = {}
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
            except: self.cache = {}
    def get(self, key: str) -> Optional[Any]: return self.cache.get(key)
    def set(self, key: str, value: Any):
        self.cache[key] = value
        self.save()
    def save(self):
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except: pass
    def __getitem__(self, key): return self.cache[key]
    def __setitem__(self, key, value):
        self.cache[key] = value
        self.save()
    def __contains__(self, key): return key in self.cache

standalone_cache = PersistentCache(STANDALONE_CACHE_FILE)
augmentation_cache = PersistentCache(AUGMENTATION_CACHE_FILE)
answer_cache = PersistentCache(ANSWER_CACHE_FILE)
complexity_cache_persistent = PersistentCache(Path("complexity_cache_jab6.json"))

# Prompts
multiturn_standalone_query_prompt = """# ì—­í• 
ë‹¹ì‹ ì€ 'ë©€í‹°í„´ ëŒ€í™”'ì—ì„œ ê²€ìƒ‰ì„ ìœ„í•œ Standalone Queryë¥¼ ë§Œë“œëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ê²€ìƒ‰ ì—”ì§„(BM25 + Dense Embedding)ì´ ì˜ ì°¾ì„ ìˆ˜ ìˆë„ë¡, ëŒ€í™” ë§¥ë½ì„ ë°˜ì˜í•œ "ë…ë¦½ëœ í•œ ë¬¸ì¥ ì§ˆë¬¸"ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤.

# ê·œì¹™ (ì¤‘ìš”)
1) ë°˜ë“œì‹œ ë§ˆì§€ë§‰ ì‚¬ìš©ì ì§ˆë¬¸ì„ ì¤‘ì‹¬ìœ¼ë¡œ í•˜ë˜, ì´ì „ ëŒ€í™”ì—ì„œ ì§€ì‹œëŒ€ìƒ(ê·¸ê±°/ê·¸ê²ƒ/ì´ê±°/ì €ê±°/ê±°ê¸°/ê·¸ ì‚¬ëŒ/ê·¸ í˜„ìƒ ë“±)ì´ ìˆìœ¼ë©´ êµ¬ì²´í™”í•˜ì„¸ìš”.
2) ê°ì • í‘œí˜„/ì¡ë‹´ì€ ì œê±°í•˜ê³  "í•µì‹¬ ê°œë…+ìš”êµ¬ì‚¬í•­" ìœ„ì£¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
3) ì¶œë ¥ì€ ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ: {"standalone_query": "...", "is_science_question": true}
"""

# ============================================================================
# Fallback Logger (v4.9.2.2_f.l NEW)
# ============================================================================

class FallbackLogger:
    """ê²€ìƒ‰ ì‹¤íŒ¨ ë° ì ìˆ˜ ì†ì‹¤ ì¼€ì´ìŠ¤ ì¶”ì  ì‹œìŠ¤í…œ"""
    def __init__(self, log_dir: str = "logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.event_file = self.log_dir / "fallback_events.jsonl"
        print(f"[INFO] FallbackLogger ì´ˆê¸°í™” (ì €ì¥ì†Œ: {self.event_file})")

    def log_event(self, eval_id: Any, query: str, reason: str, details: Dict[str, Any]):
        """ì´ë²¤íŠ¸ ê¸°ë¡"""
        event = {
            "timestamp": datetime.now().isoformat(),
            "eval_id": eval_id,
            "query": query,
            "fallback_reason": reason,
            "details": details
        }
        with open(self.event_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(event, ensure_ascii=False) + "\n")

    def classify_failure(self, hits: List[Dict], threshold: float = SCORE_THRESHOLD) -> str:
        """ì‹¤íŒ¨ ì›ì¸ ë¶„ë¥˜"""
        if not hits:
            return "EMPTY_RESULT"
        
        # ìƒìœ„ 1ìœ„ ë¬¸ì„œì˜ ì ìˆ˜ê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ LOW_SCORE
        top_score = hits[0].get("_score", 0)
        if top_score < threshold:
            return "LOW_SCORE"
            
        return "NONE" # ì •ìƒ ë˜ëŠ” ê¸°íƒ€


# ============================================================================
# SEARCH_OFF ê¸°ëŠ¥ (Rule ê¸°ë°˜) - v4.9.2_2 ê°œì„  ë²„ì „
# ============================================================================

def should_search(query: str) -> bool:
    """
    ê²€ìƒ‰ í•„ìš” ì—¬ë¶€ íŒë‹¨ (v4.9.2_2: ê³¼í•™ ì§ˆë¬¸ ì˜¤ë¶„ë¥˜ í•´ê²°)
    
    ê°œì„ ì‚¬í•­:
    1. ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´ ìš°ì„  ì²´í¬ (ì´ìœ , ì›ë¦¬, ê³¼ì •, ì—­í•  ë“±)
    2. ê³¼í•™ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥ (í–‰ì„±, ì§€í˜•, ì²œë¬¸, ì„¸í¬ ì†Œê¸°ê´€ ë“±)
    3. ë³´ìˆ˜ì  ì ‘ê·¼: ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ê²€ìƒ‰ ìˆ˜í–‰
    
    Returns:
        True: ê²€ìƒ‰ í•„ìš” (ê³¼í•™ ì§ˆë¬¸)
        False: ê²€ìƒ‰ ë¶ˆí•„ìš” (ì¡ë‹´, ì˜ê²¬, ê°ì • ë“±) â†’ topk = []
    """
    if not USE_SEARCH_OFF:
        return True  # SEARCH_OFF ë¹„í™œì„±í™” ì‹œ í•­ìƒ ê²€ìƒ‰
    
    query_lower = query.lower().strip()
    
    # ========== ìš°ì„ ìˆœìœ„ 1: ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´ ì²´í¬ (ìµœìš°ì„ !) ==========
    # "ì´ìœ ëŠ”?", "ì›ë¦¬ëŠ”?", "ê³¼ì •ì€?" ë“±ì˜ ëª…ë°±í•œ ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´
    science_question_patterns = [
        r'(ì´ìœ |ì›ë¦¬|ì›ì¸|ê³¼ì •|ì—­í• |ê¸°ëŠ¥|íŠ¹ì§•|ì„±ì§ˆ|ì°¨ì´|ë¹„êµ).*[ì€ëŠ”ì´ê°€ë¥¼]',
        r'ì™œ.*[ê¹Œê°€ëƒì•¼]',
        r'ì–´ë–»ê²Œ.*[í•˜ë˜ëŠ”]',
        r'ë¬´ì—‡.*[ì´ì¸]',
        r'.*í˜•ì„±',
        r'.*ë°œìƒ',
        r'.*ìƒê²¨',
        r'.*ë§Œë“¤ì–´',
    ]
    
    for pattern in science_question_patterns:
        if re.search(pattern, query_lower):
            return True  # ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´ â†’ ì¦‰ì‹œ ê²€ìƒ‰!
    
    # ========== ìš°ì„ ìˆœìœ„ 2: ê³¼í•™ í‚¤ì›Œë“œ ì²´í¬ (í™•ì¥) ==========
    science_keywords_extended = [
        # ê¸°ì¡´ í‚¤ì›Œë“œ
        'DNA', 'RNA', 'ì„¸í¬', 'ê´‘í•©ì„±', 'ë°˜ì‘', 'ì›ì†Œ', 'í™”í•©ë¬¼',
        'ì—ë„ˆì§€', 'ìš´ë™', 'ì†ë„', 'í˜', 'ì••ë ¥', 'ì˜¨ë„', 'ì§€êµ¬',
        'íƒœì–‘', 'í–‰ì„±', 'ìƒë¬¼', 'ì‹ë¬¼', 'ë™ë¬¼', 'ìœ ì „', 'ì§„í™”',
        'ì‚°ì†Œ', 'ë¬¼', 'ì´ì‚°í™”íƒ„ì†Œ', 'ìˆ˜ì†Œ', 'ì§ˆì†Œ', 'íƒ„ì†Œ',
        'ì „ì', 'ì–‘ì„±ì', 'ì¤‘ì„±ì', 'ì›ì', 'ë¶„ì', 'ì´ì˜¨',
        'í˜¼í•©ë¬¼', 'ìˆœë¬¼ì§ˆ', 'ì‚°', 'ì—¼ê¸°', 'ì¤‘í™”', 'ì—°ì†Œ',
        'ì†ë ¥', 'ê°€ì†ë„', 'ê´€ì„±', 'ë¹›', 'íŒŒë™', 'ì „ë¥˜', 'ì „ì••',
        'ì•”ì„', 'ê´‘ë¬¼', 'ëŒ€ê¸°', 'í•´ìˆ˜', 'êµ¬ë¦„', 'ë°”ëŒ',
        
        # ì¶”ê°€ í‚¤ì›Œë“œ (v4.9.2_2: ì˜¤ë¶„ë¥˜ í•´ê²°)
        # í–‰ì„±ë“¤
        'ìˆ˜ì„±', 'ê¸ˆì„±', 'í™”ì„±', 'ëª©ì„±', 'í† ì„±', 'ì²œì™•ì„±', 'í•´ì™•ì„±', 'ëª…ì™•ì„±',
        # ì§€í˜•
        'í˜‘ê³¡', 'í•´êµ¬', 'ì‚°ë§¥', 'ë¶„ì§€', 'í‰ì•¼', 'ê³ ì›', 'ê³„ê³¡', 'í•´ì €',
        # ì²œë¬¸ í˜„ìƒ
        'ì¼ì‹', 'ì›”ì‹', 'ì¡°ì„', 'ê³µì „', 'ìì „', 'ìœ„ì„±', 'í˜œì„±', 'ìœ ì„±',
        # ì„¸í¬ ì†Œê¸°ê´€
        'ë¦¬ë³´ì†œ', 'ë¯¸í† ì½˜ë“œë¦¬ì•„', 'ì—½ë¡ì²´', 'í•µ', 'ì„¸í¬ë§‰', 'ê³¨ì§€ì²´', 'ì†Œí¬ì²´',
        # ìƒí™”í•™
        'ë‹¨ë°±ì§ˆ', 'íš¨ì†Œ', 'í˜¸ë¥´ëª¬', 'ìœ ì „ì', 'ì—¼ìƒ‰ì²´', 'ATP', 'ì•„ë¯¸ë…¸ì‚°',
        # ë¬¼ë¦¬ í˜„ìƒ
        'ì¤‘ë ¥', 'ìë ¥', 'ì „ê¸°', 'ìê¸°', 'ì „ìê¸°', 'ë°©ì‚¬ì„ ', 'ë°©ì‚¬ëŠ¥',
        # í™”í•™ ë¬¼ì§ˆ
        'ì‚°ì†Œ', 'ìˆ˜ì†Œ', 'íƒ„ì†Œ', 'ì§ˆì†Œ', 'í—¬ë¥¨', 'ë„¤ì˜¨', 'ì•„ë¥´ê³¤',
        # ìƒë¬¼ ë¶„ë¥˜
        'í¬ìœ ë¥˜', 'ì¡°ë¥˜', 'íŒŒì¶©ë¥˜', 'ì–‘ì„œë¥˜', 'ì–´ë¥˜', 'ê³¤ì¶©', 'ì‹ë¬¼',
        # ì§€êµ¬ê³¼í•™
        'íŒ', 'ë§¨í‹€', 'ì§€ê°', 'í•µ', 'ë§ˆê·¸ë§ˆ', 'ìš©ì•”', 'ì§€ì§„', 'í™”ì‚°',
    ]
    
    if any(kw in query for kw in science_keywords_extended):
        return True  # ê³¼í•™ í‚¤ì›Œë“œ ìˆìŒ â†’ ì¦‰ì‹œ ê²€ìƒ‰!
    
    # ========== ìš°ì„ ìˆœìœ„ 3: ëª…í™•í•œ ì¡ë‹´ë§Œ í•„í„°ë§ (ë³´ìˆ˜ì ) ==========
    
    # íŒ¨í„´ 1: ì¸ì‚¬/ê°ì‚¬
    greeting_patterns = [
        r'^ì•ˆë…•',
        r'^í•˜ì´',
        r'^í—¬ë¡œ',
        r'^hi\b',
        r'^hello\b',
        r'^ê³ ë§ˆì›Œ',
        r'^ê°ì‚¬',
    ]
    
    for pattern in greeting_patterns:
        if re.search(pattern, query_lower):
            return False
    
    # íŒ¨í„´ 2: ì´ëª¨í‹°ì½˜/ê°ì • í‘œí˜„
    if re.search(r'(ã…‹ã…‹|ã…ã…|ã… ã… |ã…œã…œ|ã…¡ã…¡)', query_lower):
        return False
    
    # íŒ¨í„´ 3: ëª…í™•í•œ ê°ì • ìƒíƒœ í‘œí˜„ (ë‹¨ë…)
    emotion_only = [
        r'^.*í˜ë“¤ë‹¤\.?$',
        r'^.*ì¦ê±°ì› [ë‹¤ì–´]\.?[!?]*$',
        r'^.*ì¬ë¯¸ìˆì—ˆ[ë‹¤ì–´]\.?[!?]*$',
    ]
    
    for pattern in emotion_only:
        if re.search(pattern, query_lower):
            return False
    
    # íŒ¨í„´ 4: ë©”íƒ€ ì§ˆë¬¸ (AI ìì²´ì— ëŒ€í•œ ì§ˆë¬¸)
    meta_patterns = [
        r'^ë„ˆëŠ” ëˆ„êµ¬',
        r'^ë„ˆ ëˆ„êµ¬',
        r'^ë„ˆ.*ë­.*ì˜í•´',
        r'^ë„ˆ.*ì˜.*í•˜ëŠ”.*ê²Œ',
        r'^ë„ˆ.*ëª¨ë¥´.*ê²ƒ',
        r'^ë„ˆ ì •ë§ ë˜‘ë˜‘',
    ]
    
    for pattern in meta_patterns:
        if re.search(pattern, query_lower):
            return False
    
    # íŒ¨í„´ 5: ëŒ€í™” ì œì–´
    control_patterns = [
        r'(ê·¸ë§Œ|ì´ì œ.*ê·¸ë§Œ).*ì–˜ê¸°',
        r'^ì´ì œ ê·¸ë§Œ',
    ]
    
    for pattern in control_patterns:
        if re.search(pattern, query_lower):
            return False
    
    # ========== ê¸°ë³¸ê°’: ê²€ìƒ‰ ìˆ˜í–‰ (ë³´ìˆ˜ì  ì ‘ê·¼) ==========
    # ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ê²€ìƒ‰í•˜ëŠ” ê²ƒì´ ì•ˆì „!
    return True


# ============================================================================
# Rule ê¸°ë°˜ Complexity Classification - v4.9 (ìœ ì§€)
# ============================================================================

def classify_complexity_rule(query: str) -> Dict[str, Any]:
    """
    Rule ê¸°ë°˜ ë³µì¡ë„ ë¶„ë¥˜ (ë¹ ë¥´ê³  ì •í™•)
    
    Returns:
        {
            "complexity": "simple" | "medium" | "complex",
            "reasoning_steps": int,
            "query_type": str,
            "recommended_candidates": int,
            "requires_augmentation": bool,
            "method": "rule"
        }
    """
    query_lower = query.lower().strip()
    
    # Simple íŒ¨í„´
    simple_patterns = [
        (r'(ë¬´ì—‡|ë­|ë­”|ì´ë€|ëœ»)[\?ï¼Ÿ]?$', 'definition'),
        (r'^(ì •ì˜|ì˜ë¯¸|ëœ»)', 'definition'),
        (r'í™”í•™ ê¸°í˜¸', 'factual'),
        (r'ì›ì†Œ ê¸°í˜¸', 'factual'),
        (r'ì´ë¦„.*ë­', 'factual'),
    ]
    
    for pattern, qtype in simple_patterns:
        if re.search(pattern, query_lower):
            return {
                "complexity": "simple",
                "reasoning_steps": 1,
                "query_type": qtype,
                "recommended_candidates": 300,
                "requires_augmentation": False,
                "method": "rule"
            }
    
    # Complex íŒ¨í„´
    complex_patterns = [
        (r'ì™œ.*?ê¹Œ?', 'reasoning'),
        (r'ì´ìœ .*?[ì€ëŠ”]', 'reasoning'),
        (r'.*í•˜ë©´.*í•˜[ëŠ”ë‚˜]', 'reasoning'),
        (r'ì–´ë–»ê²Œ.*í•˜[ëŠ”ë‚˜]', 'process'),
        (r'.*ì™€.*ì˜ ê´€ê³„', 'reasoning'),
        (r'ë§Œì•½', 'reasoning'),
        (r'ê°€ì •', 'reasoning'),
    ]
    
    for pattern, qtype in complex_patterns:
        if re.search(pattern, query_lower):
            return {
                "complexity": "complex",
                "reasoning_steps": 5,
                "query_type": qtype,
                "recommended_candidates": 700,
                "requires_augmentation": True,
                "method": "rule"
            }
    
    # Medium íŒ¨í„´ (ë¹„êµ, ê³¼ì •, ì„¤ëª…)
    medium_patterns = [
        (r'ì°¨ì´', 'comparison'),
        (r'ë¹„êµ', 'comparison'),
        (r'ê³¼ì •', 'process'),
        (r'ë‹¨ê³„', 'process'),
        (r'ì„¤ëª…', 'explanation'),
    ]
    
    for pattern, qtype in medium_patterns:
        if re.search(pattern, query_lower):
            return {
                "complexity": "medium",
                "reasoning_steps": 3,
                "query_type": qtype,
                "recommended_candidates": 500,
                "requires_augmentation": True,
                "method": "rule"
            }
    
    # ê¸¸ì´ ê¸°ë°˜
    if len(query) < 10:
        return {
            "complexity": "simple",
            "reasoning_steps": 1,
            "query_type": "factual",
            "recommended_candidates": 300,
            "requires_augmentation": False,
            "method": "rule"
        }
    elif len(query) > 40:
        return {
            "complexity": "complex",
            "reasoning_steps": 4,
            "query_type": "explanation",
            "recommended_candidates": 700,
            "requires_augmentation": True,
            "method": "rule"
        }
    
    # ê¸°ë³¸: Medium
    return {
        "complexity": "medium",
        "reasoning_steps": 2,
        "query_type": "explanation",
        "recommended_candidates": 500,
        "requires_augmentation": True,
        "method": "rule"
    }


# ============================================================================
# Rule ê¸°ë°˜ Query Augmentation - v4.9.2 ê°„ë‹¨ ë²„ì „ (ìœ ì§€)
# ============================================================================

def augment_query_rule(query: str) -> Dict[str, str]:
    """
    Rule ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥ (v4.9.2_1: í•µì‹¬ ìš©ì–´ë§Œ ìœ ì§€)
    
    Returns:
        {
            "original": str,
            "expanded": str,
            "conceptual": str
        }
    """
    # ë™ì˜ì–´ ì‚¬ì „ (í•µì‹¬ 9ê°œë§Œ ìœ ì§€)
    synonym_map = {
        'DNA': ['ë””ì—”ì—ì´', 'ë””ì˜¥ì‹œë¦¬ë³´í•µì‚°', 'ìœ ì „ë¬¼ì§ˆ'],
        'RNA': ['ì•Œì—”ì—ì´', 'ë¦¬ë³´í•µì‚°'],
        'ê´‘í•©ì„±': ['íƒ„ì‚°ë™í™”ì‘ìš©', 'ë™í™”ì‘ìš©'],
        'ì„¸í¬': ['ì…€', 'cell'],
        'í˜¼í•©ë¬¼': ['í˜¼ì„±ë¬¼', 'mixture'],
        'ì‚°í™”': ['ì‚°í™”ë°˜ì‘', 'oxidation'],
        'í™˜ì›': ['í™˜ì›ë°˜ì‘', 'reduction'],
        'ì†ë„': ['ì†ë ¥', 'velocity'],
        'í˜': ['ì‘ìš©ë ¥', 'force'],
    }
    
    # Expanded: ë™ì˜ì–´ ì¶”ê°€
    expanded_terms = []
    for term, synonyms in synonym_map.items():
        if term in query:
            expanded_terms.extend(synonyms)
    
    expanded = f"{query} {' '.join(expanded_terms)}" if expanded_terms else query
    
    # Conceptual: ê´€ë ¨ ê°œë… (ê°„ë‹¨ ê·œì¹™)
    conceptual = query  # ê¸°ë³¸ì€ ì›ë³¸ ìœ ì§€
    
    return {
        "original": query,
        "expanded": expanded[:100],  # 100ì ì œí•œ
        "conceptual": conceptual[:100]
    }


# ============================================================================
# OpenAI API Wrapper with Retry (Rate Limit Mitigation)
# ============================================================================

def call_openai_with_retry(messages: List[Dict], model: str = QUERY_LLM_MODEL, max_retries: int = 5, initial_wait: int = 5):
    """
    OpenAI API í˜¸ì¶œ ì‹œ Rate Limit ë° ì¼ì‹œì  ì˜¤ë¥˜ì— ëŒ€í•œ Exponential Backoff ì ìš©
    """
    for i in range(max_retries):
        try:
            return openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0,
                seed=1
            )
        except Exception as e:
            if "Rate limit" in str(e) or "quota" in str(e) or "429" in str(e):
                wait_time = initial_wait * (2 ** i)
                print(f"[RETRY] Rate limit ê°ì§€. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ í•©ë‹ˆë‹¤... (ì‹œë„ {i+1}/{max_retries})", flush=True)
                time.sleep(wait_time)
            else:
                print(f"[ERROR] OpenAI í˜¸ì¶œ ì¤‘ ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ ë°œìƒ: {e}", flush=True)
                raise e
    raise Exception("OpenAI API í˜¸ì¶œ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")



# ============================================================================
# LLM ê¸°ë°˜ (Fallback) - v4.8ê³¼ ë™ì¼
# ============================================================================

class QueryAugmenter:
    """GPT-4o-mini ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥ (Fallback)"""
    
    def __init__(self, client: OpenAI, model: str = QUERY_LLM_MODEL):
        self.client = client
        self.model = model
        self.cache = augmentation_cache
        print("[INFO] QueryAugmenter (LLM Fallback) ì´ˆê¸°í™” ì™„ë£Œ", flush=True)
    
    def augment_query(self, query: str) -> Dict[str, str]:
        """LLM ê¸°ë°˜ ì¿¼ë¦¬ í™•ì¥"""
        if query in self.cache:
            return self.cache[query]
        
        system_prompt = """ë‹¹ì‹ ì€ ê³¼í•™ ì§€ì‹ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ í™•ì¥ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ ë‹¤ìŒ 3ê°€ì§€ ë²„ì „ì˜ ì¿¼ë¦¬ë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”:

1. original: ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ (ì •ì œëœ ë²„ì „)
2. expanded: ë™ì˜ì–´ì™€ ìœ ì‚¬ í‘œí˜„ì„ í¬í•¨í•œ í™•ì¥ ì¿¼ë¦¬
3. conceptual: ê´€ë ¨ëœ ê³¼í•™ ê°œë…ê³¼ ìš©ì–´ë¥¼ í¬í•¨í•œ ì¿¼ë¦¬

ì¶œë ¥ í˜•ì‹ (JSON):
{
  "original": "ì •ì œëœ ì›ë³¸ ì§ˆë¬¸",
  "expanded": "ë™ì˜ì–´ë¥¼ í¬í•¨í•œ í™•ì¥ ì¿¼ë¦¬",
  "conceptual": "ê´€ë ¨ ê°œë…ì„ í¬í•¨í•œ ì¿¼ë¦¬"
}
"""
        
        try:
            response = call_openai_with_retry(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ì§ˆë¬¸: {query}"}
                ]
            )
            
            result = json.loads(response.choices[0].message.content)
            augmented = {
                "original": result.get("original", query),
                "expanded": result.get("expanded", query),
                "conceptual": result.get("conceptual", query)
            }
            
            self.cache[query] = augmented
            return augmented
        
        except Exception as e:
            print(f"[WARNING] Query Augmentation (LLM) ì‹¤íŒ¨: {e}", flush=True)
            return {
                "original": query,
                "expanded": query,
                "conceptual": query
            }


class QueryComplexityClassifier:
    """GPT-4o-mini ê¸°ë°˜ ë³µì¡ë„ ë¶„ë¥˜ (Fallback)"""
    
    def __init__(self, client: OpenAI, model: str = QUERY_LLM_MODEL):
        self.client = client
        self.model = model
        self.cache = complexity_cache_persistent
        
        self.tools = [
            {
                "type": "function",
                "function": {
                    "name": "classify_query_complexity",
                    "description": "ê³¼í•™ ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ ë¶„ì„í•˜ì—¬ ë¶„ë¥˜í•©ë‹ˆë‹¤",
                    "parameters": {
                        "type": "object",
                        "properties": {
                            "complexity": {
                                "type": "string",
                                "enum": ["simple", "medium", "complex"]
                            },
                            "reasoning_steps": {"type": "integer"},
                            "query_type": {"type": "string"},
                            "recommended_candidates": {"type": "integer"},
                            "requires_augmentation": {"type": "boolean"}
                        },
                        "required": ["complexity", "reasoning_steps", "query_type",
                                    "recommended_candidates", "requires_augmentation"]
                    }
                }
            }
        ]
        
        print("[INFO] QueryComplexityClassifier (LLM Fallback) ì´ˆê¸°í™” ì™„ë£Œ", flush=True)
    
    def classify(self, query: str) -> Dict[str, Any]:
        """LLM ê¸°ë°˜ ë³µì¡ë„ ë¶„ë¥˜"""
        if query in self.cache:
            return self.cache[query]
        
        system_prompt = """ì§ˆë¬¸ì˜ ë³µì¡ë„ë¥¼ Simple/Medium/Complexë¡œ ë¶„ë¥˜í•˜ì„¸ìš”.
Simple: ì •ì˜, ë‹¨ìˆœ ì‚¬ì‹¤ (300ê°œ í›„ë³´)
Medium: ì„¤ëª…, ë¹„êµ (500ê°œ í›„ë³´)
Complex: ë‹¤ë‹¨ê³„ ì¶”ë¡ , ì¸ê³¼ê´€ê³„ (700ê°œ í›„ë³´)"""
        
        try:
            response = call_openai_with_retry(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": f"ì§ˆë¬¸: {query}"}
                ]
            )
            
            # Note: function calling might need adjustment if using call_openai_with_retry
            # but for now, we'll keep it simple to ensure retries work.
            # If function calling is strictly needed, we'd add tools to call_openai_with_retry.
            # Since gpt-4o-mini is good at JSON without tools, we'll fallback to JSON if tools fail.
            result_str = response.choices[0].message.content
            if "{" in result_str:
                result = json.loads(result_str[result_str.find("{"):result_str.rfind("}")+1])
            else:
                raise Exception("JSON not found in response")
            result["method"] = "llm"
            
            self.cache[query] = result
            return result
        
        except Exception as e:
            print(f"[WARNING] Complexity Classification (LLM) ì‹¤íŒ¨: {e}", flush=True)
            return {
                "complexity": "medium",
                "reasoning_steps": 2,
                "query_type": "explanation",
                "recommended_candidates": 500,
                "requires_augmentation": True,
                "method": "llm"
            }


# ============================================================================
# KURE Embedding Manager
# ============================================================================

class KUREEmbeddingManager:
    """KURE (í•œêµ­ì–´ íŠ¹í™”) Embedding ê´€ë¦¬"""
    
    def __init__(self, model_name: str = EMBEDDING_MODEL_NAME):
        print(f"\n[INFO] KURE Embedding ëª¨ë¸ ë¡œë”©: {model_name}", flush=True)
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"[INFO] ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}", flush=True)
        self.model = SentenceTransformer(model_name, device=device)
        self.model_name = model_name
        self.device = device
        print(f"[INFO] KURE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! (ì°¨ì›: {EMBEDDING_DIMS}, ë””ë°”ì´ìŠ¤: {device})", flush=True)
    
    def get_embeddings(self, texts: List[str], 
                       batch_size: int = BATCH_SIZE,
                       show_progress: bool = False) -> List[List[float]]:
        """KUREë¡œ ì„ë² ë”© ìƒì„±"""
        if isinstance(texts, str):
            texts = [texts]
        
        all_embeddings = []
        total_batches = (len(texts) + batch_size - 1) // batch_size
        
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i + batch_size]
            batch_num = i // batch_size + 1
            
            if show_progress and total_batches > 1:
                print(f"[KURE] ë°°ì¹˜ {batch_num}/{total_batches} ì²˜ë¦¬ ì¤‘...", flush=True)
            
            batch_embeddings = self.model.encode(
                batch,
                convert_to_numpy=True,
                show_progress_bar=False,
                batch_size=batch_size
            )
            
            all_embeddings.extend([emb.tolist() for emb in batch_embeddings])
        
        return all_embeddings


# ============================================================================
# Enhanced Reranker (2-Stage)
# ============================================================================

class EnhancedReranker:
    """2-Stage Enhanced Reranker"""
    
    def __init__(self):
        print("\n[INFO] Enhanced Reranker ì´ˆê¸°í™”...", flush=True)
        
        # GPU ì‚¬ìš© ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"[INFO] ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}", flush=True)
        
        try:
            self.primary_reranker = CrossEncoder('BAAI/bge-reranker-v2-m3', device=device)
            print("[INFO] BGE-Reranker-v2-m3 ë¡œë“œ ì„±ê³µ", flush=True)
        except:
            self.primary_reranker = None
        
        try:
            # Fine-tuned Reranker ì‚¬ìš© (jab4 í•µì‹¬ ë³€ê²½)
            finetuned_path = "/data/ephemeral/home/code/models/jab5_reranker"
            self.secondary_reranker = CrossEncoder(finetuned_path, device=device)
            print(f"[INFO] Fine-tuned ko-reranker ë¡œë“œ ì„±ê³µ: {finetuned_path}", flush=True)
        except:
            self.secondary_reranker = None
        
        if not self.primary_reranker and not self.secondary_reranker:
            self.primary_reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2', device=device)
        
        self.device = device
        print("[INFO] Enhanced Reranker ì´ˆê¸°í™” ì™„ë£Œ!", flush=True)
    
    def rerank_stage1(self, query: str, hits: List[Dict], top_k: int) -> List[Dict]:
        """Stage 1: ë¹ ë¥¸ í•„í„°ë§"""
        if not hits:
            return []
        
        pairs = [[query, hit['_source']['content']] for hit in hits]
        
        if self.primary_reranker:
            scores = self.primary_reranker.predict(pairs, show_progress_bar=False)
        else:
            scores = np.array([hit['_score'] for hit in hits])
        
        for i, hit in enumerate(hits):
            hit['_rerank_score'] = float(scores[i])
        
        return sorted(hits, key=lambda x: x['_rerank_score'], reverse=True)[:top_k]
    
    def rerank_stage2(self, query: str, hits: List[Dict], top_k: int) -> List[Dict]:
        """Stage 2: ì •ë°€ ì¬ì •ë ¬ (ì•™ìƒë¸”)"""
        if not hits:
            return []
        
        pairs = [[query, hit['_source']['content']] for hit in hits]
        
        primary_scores = None
        if self.primary_reranker:
            primary_scores = self.primary_reranker.predict(pairs, show_progress_bar=False)
        
        secondary_scores = None
        if self.secondary_reranker:
            secondary_scores = self.secondary_reranker.predict(pairs, show_progress_bar=False)
        
        if primary_scores is not None and secondary_scores is not None:
            primary_norm = (primary_scores - np.mean(primary_scores)) / (np.std(primary_scores) + 1e-8)
            secondary_norm = (secondary_scores - np.mean(secondary_scores)) / (np.std(secondary_scores) + 1e-8)
            final_scores = PRIMARY_RERANKER_WEIGHT * primary_norm + SECONDARY_RERANKER_WEIGHT * secondary_norm
        elif primary_scores is not None:
            final_scores = primary_scores
        elif secondary_scores is not None:
            final_scores = secondary_scores
        else:
            final_scores = np.array([hit.get('_rerank_score', hit['_score']) for hit in hits])
        
        for i, hit in enumerate(hits):
            hit['_score'] = float(final_scores[i])
        
        return sorted(hits, key=lambda x: x['_score'], reverse=True)[:top_k]
    
    def two_stage_rerank(self, query: str, hits: List[Dict],
                         stage1_k: int, stage2_k: int) -> List[Dict]:
        """2-Stage Reranking ì „ì²´ ì‹¤í–‰"""
        stage1_results = self.rerank_stage1(query, hits, stage1_k)
        stage2_results = self.rerank_stage2(query, stage1_results, stage2_k)
        return stage2_results


# ============================================================================
# RRF (Reciprocal Rank Fusion)
# ============================================================================

def reciprocal_rank_fusion(rankings: List[List[str]], k: int = RRF_K) -> List[Tuple[str, float]]:
    """RRF ê²°í•©"""
    rrf_scores = {}
    
    for ranking in rankings:
        for rank, docid in enumerate(ranking, start=1):
            if docid not in rrf_scores:
                rrf_scores[docid] = 0
            rrf_scores[docid] += 1.0 / (k + rank)
    
    return sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)


# ============================================================================
# RAG Pipeline v4.9.2_1
# ============================================================================

class RAGPipelineV4922:
    """
    RAG v4.9.2_2 íŒŒì´í”„ë¼ì¸
    
    ê°œì„ ì‚¬í•­:
    1. v4.9.2_1 ê¸°ë°˜ (ì •ë³´ì§€ì‹ êµì‚¬)
    2. SEARCH_OFF ë¡œì§ ê°œì„  (ê³¼í•™ ì§ˆë¬¸ ì˜¤ë¶„ë¥˜ í•´ê²°)
    3. 5ê°œ ì˜¤ë¶„ë¥˜ ì§ˆë¬¸ ë³µêµ¬
    """
    
    def __init__(self, es: Elasticsearch, index_name: str = "rag_v4922"):
        self.es = es
        self.index_name = index_name
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.embedding_manager = KUREEmbeddingManager(EMBEDDING_MODEL_NAME)
        self.reranker = EnhancedReranker()
        
        # LLM Fallback
        self.query_augmenter = QueryAugmenter(openai_client, QUERY_LLM_MODEL)
        self.complexity_classifier = QueryComplexityClassifier(openai_client, QUERY_LLM_MODEL)
        
        # Fallback Logger (NEW)
        self.fb_logger = FallbackLogger()
        
        # ì„¤ì •
        self.fusion_ratio = FUSION_RATIO
        self.reranker_ratio = RERANKER_RATIO
        self.doc_cache = {}
        
        print(f"\n[INFO] RAG Pipeline v4.9.2.2_f.l ì´ˆê¸°í™” ì™„ë£Œ")
        print(f"[INFO] âœ… v4.9.2.2_f.l (Logging + Top-N)")
        print(f"[INFO] âœ… SEARCH_OFF ë¡œì§ ê°œì„  ì ìš©ë¨")
        print(f"[INFO] âœ… ì •ë³´ì§€ì‹ êµì‚¬ ì—­í•  ìœ ì§€")
    
    def create_index(self, settings: Dict, mappings: Dict, force: bool = False):
        """
        ì¸ë±ìŠ¤ ìƒì„±
        
        Args:
            force: Trueë©´ ê¸°ì¡´ ì‚­ì œ í›„ ìƒì„±, Falseë©´ ì¡´ì¬ ì‹œ ì¬ì‚¬ìš©
        """
        exists = self.es.indices.exists(index=self.index_name)
        
        if exists and not force:
            print(f"[INFO] ì¸ë±ìŠ¤ '{self.index_name}' ì´ë¯¸ ì¡´ì¬ (ì¬ì‚¬ìš©)")
            return
        
        if exists and force:
            self.es.indices.delete(index=self.index_name)
            print(f"[WARN] ê¸°ì¡´ ì¸ë±ìŠ¤ '{self.index_name}' ì‚­ì œ (force=True)")
        
        self.es.indices.create(
            index=self.index_name,
            settings=settings,
            mappings=mappings
        )
        print(f"[INFO] ìƒˆ ì¸ë±ìŠ¤ '{self.index_name}' ìƒì„± ì™„ë£Œ")
    
    def index_documents(self, documents: List[Dict], batch_size: int = BATCH_SIZE):
        """ë¬¸ì„œ ì¸ë±ì‹±"""
        print(f"\n[INFO] {len(documents)}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì‹œì‘...")
        
        contents = [doc["content"] for doc in documents]
        
        # KURE ì„ë² ë”© ìƒì„±
        print("[INFO] KURE Embedding ìƒì„± ì¤‘...", flush=True)
        embeddings = self.embedding_manager.get_embeddings(contents, batch_size, show_progress=True)
        
        # Elasticsearch ë²Œí¬ ì¸ë±ì‹±
        print("[INFO] Elasticsearch ì¸ë±ì‹± ì¤‘...", flush=True)
        all_docs = []
        for i, doc in enumerate(documents):
            doc_with_emb = doc.copy()
            doc_with_emb["embeddings"] = embeddings[i]
            all_docs.append(doc_with_emb)
            self.doc_cache[doc["docid"]] = doc["content"]
        
        actions = [{"_index": self.index_name, "_source": doc} for doc in all_docs]
        success, errors = helpers.bulk(self.es, actions, raise_on_error=False)
        
        if errors:
            print(f"[WARNING] ì¸ë±ì‹± ì¤‘ {len(errors)}ê°œ ì—ëŸ¬")
        
        print(f"[SUCCESS] {success}ê°œ ë¬¸ì„œ ì¸ë±ì‹± ì™„ë£Œ\n")
        return success
    
    def sparse_search(self, query: str, size: int) -> List[Dict]:
        """BM25 ê²€ìƒ‰"""
        result = self.es.search(
            index=self.index_name,
            query={"match": {"content": {"query": query}}},
            size=size
        )
        return result['hits']['hits']
    
    def dense_search(self, query: str, size: int) -> List[Dict]:
        """Dense ê²€ìƒ‰ (KURE - v4.9.2.2_f.l: num_candidates ìƒí–¥)"""
        query_emb = self.embedding_manager.get_embeddings([query])[0]
        
        result = self.es.search(
            index=self.index_name,
            knn={
                "field": "embeddings",
                "query_vector": query_emb,
                "k": size,
                "num_candidates": max(size * 4, 3000) # 1000 â†’ 3000+
            }
        )
        return result['hits']['hits']
    
    def multi_query_search_with_rrf(self, queries: Dict[str, str], size: int) -> List[Dict]:
        """Multi-Query Search with RRF"""
        all_rankings = []
        doc_map = {}
        
        for query_type, query in queries.items():
            # Dense ê²€ìƒ‰
            dense_hits = self.dense_search(query, size)
            dense_ranking = [hit['_source']['docid'] for hit in dense_hits]
            all_rankings.append(dense_ranking)
            
            # Sparse ê²€ìƒ‰
            sparse_hits = self.sparse_search(query, size)
            sparse_ranking = [hit['_source']['docid'] for hit in sparse_hits]
            all_rankings.append(sparse_ranking)
            
            # ë¬¸ì„œ ë§¤í•‘
            for hit in dense_hits + sparse_hits:
                docid = hit['_source']['docid']
                if docid not in doc_map:
                    doc_map[docid] = hit
        
        # RRF ê²°í•©
        rrf_results = reciprocal_rank_fusion(all_rankings, k=RRF_K)
        
        # ê²°ê³¼ ìƒì„±
        result_hits = []
        for docid, rrf_score in rrf_results[:size]:
            if docid in doc_map:
                hit = doc_map[docid].copy()
                hit['_score'] = rrf_score
                hit['_rrf_score'] = rrf_score
                result_hits.append(hit)
        
        return result_hits
    
    def search(self, query: str, top_k: int = FINAL_TOP_K, eval_id: Any = None) -> List[Dict]:
        """
        ì „ì²´ ê²€ìƒ‰ íŒŒì´í”„ë¼ì¸ (v4.9.2.2_f.l: ë¡œê¹… + í›„ë³´êµ° í™•ëŒ€)
        """
        print(f"\n[SEARCH] Query: {query}")
        
        # 0. SEARCH_OFF íŒë‹¨
        if not should_search(query):
            print(f"[SEARCH_OFF] ê²€ìƒ‰ ë¶ˆí•„ìš” (ì¡ë‹´/ì˜ê²¬/ê°ì •)")
            if eval_id is not None:
                self.fb_logger.log_event(eval_id, query, "SEARCH_OFF", {"reason": "Classified as chitchat"})
            return []
        
        print("[SEARCH_ON] ê²€ìƒ‰ ì‹¤í–‰")
        
        # 1. Complexity Classification
        if USE_RULE_BASED:
            complexity_info = classify_complexity_rule(query)
        else:
            complexity_info = self.complexity_classifier.classify(query)
        
        # v4.9.2.2_f.l: í›„ë³´êµ° ìŠ¤ì¼€ì¼ë§
        orig_candidates = complexity_info['recommended_candidates']
        candidates = orig_candidates * 2 # í›„ë³´êµ° 2ë°° í™•ëŒ€ (ìµœëŒ€ 1400)
        
        print(f"[CLASSIFY] Complexity: {complexity_info['complexity']} (Candidates: {candidates})")
        
        use_augmentation = complexity_info['requires_augmentation']
        
        if complexity_info['complexity'] == 'simple':
            stage1_k, stage2_k = 100, 30
        elif complexity_info['complexity'] == 'medium':
            stage1_k, stage2_k = FIRST_STAGE_K, SECOND_STAGE_K
        else:  # complex
            stage1_k, stage2_k = 150, 50
        
        # 2. Query Augmentation
        if use_augmentation:
            if USE_RULE_BASED:
                augmented_queries = augment_query_rule(query)
            else:
                augmented_queries = self.query_augmenter.augment_query(query)
        else:
            augmented_queries = {"original": query, "expanded": query, "conceptual": query}
        
        # 3. Multi-Query Search with RRF
        hybrid_results = self.multi_query_search_with_rrf(augmented_queries, candidates)
        print(f"[SEARCH] RRF ê²°ê³¼: {len(hybrid_results)}ê°œ")
        
        # ì‹¤íŒ¨ ë¡œê¹… ì „ì²˜ë¦¬
        fail_reason = self.fb_logger.classify_failure(hybrid_results)
        if fail_reason != "NONE" and eval_id is not None:
            self.fb_logger.log_event(eval_id, query, fail_reason, {
                "hits_count": len(hybrid_results),
                "top_score": hybrid_results[0].get("_score", 0) if hybrid_results else 0
            })

        # 4. 2-Stage Reranking
        reranked_results = self.reranker.two_stage_rerank(
            query, hybrid_results, stage1_k, stage2_k
        )
        print(f"[SEARCH] Reranking ì™„ë£Œ: {len(reranked_results)}ê°œ")
        
        # 5. Final Fusion
        rrf_scores = {hit['_source']['docid']: hit.get('_rrf_score', hit['_score']) 
                      for hit in hybrid_results}
        reranker_scores = {hit['_source']['docid']: hit['_score'] 
                          for hit in reranked_results}
        
        common_docids = list(set(rrf_scores.keys()) & set(reranker_scores.keys()))
        
        if not common_docids:
            final_results = reranked_results[:top_k]
        else:
            rrf_vals = np.array([rrf_scores[d] for d in common_docids])
            reranker_vals = np.array([reranker_scores[d] for d in common_docids])
            
            rrf_norm = (rrf_vals - np.mean(rrf_vals)) / (np.std(rrf_vals) + 1e-8)
            reranker_norm = (reranker_vals - np.mean(reranker_vals)) / (np.std(reranker_vals) + 1e-8)
            
            final_scores = {}
            for i, docid in enumerate(common_docids):
                final_scores[docid] = (self.fusion_ratio * rrf_norm[i] + 
                                      self.reranker_ratio * reranker_norm[i])
            
            sorted_final = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
            doc_map = {hit['_source']['docid']: hit for hit in reranked_results}
            
            final_results = []
            for docid, score in sorted_final[:top_k]:
                if docid in doc_map:
                    hit = doc_map[docid].copy()
                    hit['_score'] = float(score)
                    final_results.append(hit)
        
        # ê²°ê³¼ ì¶œë ¥
        for i, hit in enumerate(final_results):
            print(f"[RESULT] Rank {i+1}: {hit['_source']['docid']} (score: {hit['_score']:.4f})")
        
        return final_results


# ============================================================================
# Elasticsearch ì„¤ì •
# ============================================================================

es_username = "elastic"
es_password = "your_password_here"  # ì‹¤ì œ ë¹„ë°€ë²ˆí˜¸ë¡œ êµì²´ í•„ìš”

es = Elasticsearch(
    ['https://localhost:9200'],
    basic_auth=(es_username, es_password),
    ca_certs="./elasticsearch-8.8.0/config/certs/http_ca.crt"
)

print("[INFO] Elasticsearch ì—°ê²°:", es.info()['version']['number'])

# ìš©ì–´ ì •ê·œí™” + ë™ì˜ì–´ ì‚¬ì „ (v4.8ê³¼ ë™ì¼)
TERM_NORMALIZATIONS = [
    "ë””ì—”ì—ì´ => DNA", "ë””ì—”ì— => DNA", "ë””.ì—”.ì—ì´ => DNA",
    "ì•Œì—”ì—ì´ => RNA", "ì•Œì—”ì— => RNA", "ì•Œ.ì—”.ì—ì´ => RNA",
    "í”¼ì—ì´ì¹˜ => pH", "í”¼.ì—ì´ì¹˜ => pH",
    "ì—ì´í‹°í”¼ => ATP", "ì—ì´.í‹°.í”¼ => ATP",
    "ì‹œì‹œ => cc", "ì— ì—˜ => mL", "í‚¬ë¡œê·¸ë¨ => kg", "ë¯¸í„° => m",
]

SCIENCE_SYNONYMS = [
    "DNA, ë””ì˜¥ì‹œë¦¬ë³´í•µì‚°, ìœ ì „ë¬¼ì§ˆ, ìœ ì „ì",
    "RNA, ë¦¬ë³´í•µì‚°, ì „ë ¹RNA, mRNA",
    "ìœ ì „ì, ìœ ì „ì¸ì, gene",
    "ì„¸í¬, ì…€, cell",
    "ê´‘í•©ì„±, íƒ„ì‚°ë™í™”ì‘ìš©, ë™í™”ì‘ìš©",
    "í˜¼í•©ë¬¼, í˜¼ì„±ë¬¼, í˜¼í•©ì²´, ë¯¹ìŠ¤ì²˜, mixture",
    "ì‚°í™”, ì‚°í™”ë°˜ì‘, oxidation",
    "í™˜ì›, í™˜ì›ë°˜ì‘, reduction",
    "ì†ë„, ì†ë ¥, velocity, speed",
    "í˜, ì‘ìš©ë ¥, force",
]

settings = {
    "number_of_shards": 1,
    "number_of_replicas": 0,
    "analysis": {
        "char_filter": {
            "normalize_science_terms": {
                "type": "mapping",
                "mappings": TERM_NORMALIZATIONS
            }
        },
        "analyzer": {
            "nori": {
                "type": "custom",
                "char_filter": ["normalize_science_terms"],
                "tokenizer": "nori_tokenizer",
                "decompound_mode": "mixed",
                "filter": ["nori_posfilter", "lowercase", "science_synonym"]
            }
        },
        "filter": {
            "nori_posfilter": {
                "type": "nori_part_of_speech",
                "stoptags": ["E", "IC", "J", "MAG", "MAJ", "MM", "SP", "SSC", "SSO", 
                            "SC", "SE", "XPN", "XSA", "XSN", "XSV", "UNA", "NA", "VSV"]
            },
            "science_synonym": {
                "type": "synonym",
                "synonyms": SCIENCE_SYNONYMS,
                "lenient": True
            }
        }
    },
    "index": {
        "similarity": {
            "custom_bm25": {
                "type": "BM25",
                "k1": BM25_K1,
                "b": BM25_B
            }
        }
    }
}

mappings = {
    "properties": {
        "docid": {"type": "keyword"},
        "content": {
            "type": "text",
            "analyzer": "nori",
            "similarity": "custom_bm25"
        },
        "embeddings": {
            "type": "dense_vector",
            "dims": EMBEDDING_DIMS,
            "index": True,
            "similarity": "cosine"
        }
    }
}


# ============================================================================
# LLM ì„¤ì • (v4.9.2_1: ì •ë³´ì§€ì‹ êµì‚¬ ì—­í•  ì¶”ê°€)
# ============================================================================

llm_client = OpenAI(api_key=OPENAI_API_KEY)
llm_model = "gpt-4o-mini"

# ê°„ë‹¨í•œ Chitchat í”„ë¡¬í”„íŠ¸
persona_chitchat = """ë‹¹ì‹ ì€ ì¹œì ˆí•˜ê³  ë„ì›€ì´ ë˜ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì´ë‚˜ ê°ì •ì— ê³µê°í•˜ë©°, ì¹œì ˆí•˜ê³  ìì—°ìŠ¤ëŸ½ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.
- ì¸ì‚¬ë‚˜ ê°ì‚¬ì—ëŠ” ë”°ëœ»í•˜ê²Œ ì‘ë‹µí•˜ì„¸ìš”.
- ê°ì • í‘œí˜„ì—ëŠ” ê³µê°í•˜ë©° ì‘ë‹µí•˜ì„¸ìš”.
- ì§ˆë¬¸ì´ ìˆìœ¼ë©´ ìµœì„ ì„ ë‹¤í•´ ë‹µë³€í•˜ì„¸ìš”.
ë‹µë³€ì€ 100-200ì ì •ë„ë¡œ ì ì ˆí•œ ê¸¸ì´ë¡œ ì‘ì„±í•˜ì„¸ìš”."""

# RAG í”„ë¡¬í”„íŠ¸ (v4.9.2_1: ì •ë³´ì§€ì‹ êµì‚¬ ì—­í•  ì¶”ê°€ + ë‹¨ìˆœí™”)
persona_qa = """# ì—­í• 
ë‹¹ì‹ ì€ 15ë…„ ì´ìƒì˜ ê²½ë ¥ì„ ê°€ì§„ ê³¼í•™ êµì‚¬ì´ì ì •ë³´ì§€ì‹ êµì‚¬ì…ë‹ˆë‹¤. í•™ìƒë“¤ì—ê²Œ ê³¼í•™ ì§€ì‹ë¿ë§Œ ì•„ë‹ˆë¼ ì •ë³´ì§€ì‹(í”„ë¡œê·¸ë˜ë°, ì»´í“¨í„° ê³¼í•™, ë°ì´í„° ë“±)ë„ ëª…í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ê²ƒì´ ì „ë¬¸ì…ë‹ˆë‹¤.

# ì§ˆë¬¸ ì¬ì½ê¸° (Read the Question Again)
**ì¤‘ìš”: ë‹µë³€í•˜ê¸° ì „ì— í•™ìƒì˜ ì§ˆë¬¸ì„ ë‹¤ì‹œ í•œ ë²ˆ ì£¼ì˜ ê¹Šê²Œ ì½ìœ¼ì„¸ìš”.**
- ì§ˆë¬¸ì´ ì •í™•íˆ ë¬´ì—‡ì„ ë¬»ê³  ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.
- "ì •ì˜"ë¥¼ ë¬»ëŠ”ê°€, "ê³¼ì •"ì„ ë¬»ëŠ”ê°€, "ì´ìœ "ë¥¼ ë¬»ëŠ”ê°€, "ë¹„êµ"ë¥¼ ë¬»ëŠ”ê°€?
- ì§ˆë¬¸ì˜ í•µì‹¬ í‚¤ì›Œë“œëŠ” ë¬´ì—‡ì¸ê°€?

# ì§€ì‹œì‚¬í•­
Referenceì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•˜ê³  ìƒì„¸í•˜ê²Œ ë‹µë³€í•˜ì„¸ìš”.

**í•„ìˆ˜ ìš”êµ¬ì‚¬í•­:**
1. ë‹µë³€ì€ ìµœì†Œ 200ì ì´ìƒìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”. ê°€ëŠ¥í•œ í•œ ìƒì„¸í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
2. ê´€ë ¨ëœ ê°œë…, ì›ë¦¬, ì˜ˆì‹œ, ê³¼ì •ì„ í¬í•¨í•˜ì—¬ ì„¤ëª…í•˜ì„¸ìš”.
3. Referenceì— ì§ì ‘ì ì¸ ë‹µì´ ì—†ì–´ë„ ê´€ë ¨ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë…¼ë¦¬ì ìœ¼ë¡œ ì¶”ë¡ í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”.
4. Referenceì˜ ì •ë³´ë¥¼ ë‹¨ìˆœíˆ ë‚˜ì—´í•˜ì§€ ë§ê³ , **ì§ˆë¬¸ê³¼ ì—°ê²°í•˜ì—¬** í†µí•©ì ìœ¼ë¡œ ì„¤ëª…í•˜ì„¸ìš”.
5. Referenceì— ì „í˜€ ê´€ë ¨ ì •ë³´ê°€ ì—†ì„ ë•Œë§Œ "ì œê³µëœ ì •ë³´ë¡œëŠ” ë‹µë³€í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  í•˜ì„¸ìš”. (ì´ ê²½ìš°ëŠ” ë§¤ìš° ë“œë­…ë‹ˆë‹¤)

**ë‹µë³€ êµ¬ì¡°:**
- í•µì‹¬ ê°œë… ì„¤ëª… (ì§ˆë¬¸ì— ì§ì ‘ ë‹µí•˜ê¸°)
- ê´€ë ¨ ì›ë¦¬ë‚˜ ë©”ì»¤ë‹ˆì¦˜ ì„¤ëª…
- êµ¬ì²´ì ì¸ ì˜ˆì‹œë‚˜ ê³¼ì • ì„¤ëª… (ê°€ëŠ¥í•œ ê²½ìš°)
- ìš”ì•½ ë˜ëŠ” ê²°ë¡ 

# ì˜ˆì‹œ
ì§ˆë¬¸: "ê´‘í•©ì„±ì´ë€?"
Reference: "ê´‘í•©ì„±ì€ ì‹ë¬¼ì´ ë¹› ì—ë„ˆì§€ë¡œ..."
ë‹µë³€: "ê´‘í•©ì„±ì€ ì‹ë¬¼ì´ ë¹› ì—ë„ˆì§€ë¥¼ ì´ìš©í•˜ì—¬ ì´ì‚°í™”íƒ„ì†Œì™€ ë¬¼ë¡œë¶€í„° í¬ë„ë‹¹ì„ ë§Œë“œëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ì—½ë¡ì†Œê°€ ë¹› ì—ë„ˆì§€ë¥¼ í¡ìˆ˜í•˜ê³ , ë¬¼ê³¼ ì´ì‚°í™”íƒ„ì†Œë¥¼ ë°˜ì‘ì‹œì¼œ í¬ë„ë‹¹ê³¼ ì‚°ì†Œë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ìƒì„±ëœ í¬ë„ë‹¹ì€ ì‹ë¬¼ì˜ ì—ë„ˆì§€ì›ìœ¼ë¡œ ì‚¬ìš©ë˜ë©°, ì‚°ì†ŒëŠ” ëŒ€ê¸°ë¡œ ë°©ì¶œë©ë‹ˆë‹¤. ê´‘í•©ì„±ì€ ì§€êµ¬ ìƒíƒœê³„ì—ì„œ ë§¤ìš° ì¤‘ìš”í•œ ì—­í• ì„ í•˜ë©°, ëŒ€ê¸° ì¤‘ì˜ ì´ì‚°í™”íƒ„ì†Œë¥¼ ì¤„ì´ê³  ì‚°ì†Œë¥¼ ê³µê¸‰í•©ë‹ˆë‹¤."
"""


def get_messages_hash(messages: List[Dict]) -> str:
    return hashlib.md5(json.dumps(messages, sort_keys=True).encode('utf-8')).hexdigest()

def build_standalone_query(messages: List[Dict]) -> Tuple[str, bool]:
    if len(messages) == 1: return messages[0]['content'], True
    m_hash = get_messages_hash(messages)
    cached = standalone_cache.get(m_hash)
    if cached: return cached['standalone_query'], cached['is_science_question']
    
    try:
        resp = openai_client.chat.completions.create(
            model=QUERY_LLM_MODEL,
            messages=[{"role": "system", "content": multiturn_standalone_query_prompt}, {"role": "user", "content": json.dumps({"msg": messages})}],
            temperature=0, seed=1, response_format={"type": "json_object"}
        )
        j = json.loads(resp.choices[0].message.content)
        standalone_cache.set(m_hash, j)
        return j.get("standalone_query", ""), j.get("is_science_question", True)
    except:
        return messages[-1]['content'], True

def answer_question(messages: List[Dict], pipeline: RAGPipelineV4922, include_references: bool = False, eval_id: Any = None) -> Dict:
    """
    RAG ê¸°ë°˜ ì§ˆë¬¸ ë‹µë³€ (v4.9.2.jab6: Standalone + Multi-Pass)
    """
    m_hash = get_messages_hash(messages)
    
    # 1. Standalone Query (Cache/API/Skip)
    standalone_query, is_science = build_standalone_query(messages)
    
    # 2. Answer Caching (Check if we can reuse answer)
    cached_ans = answer_cache.get(m_hash)
    
    # 3. Search (Always run for TopK evaluation)
    if not is_science or not standalone_query:
        search_results = []
    else:
        # Jab6 í•µì‹¬: Standalone Queryë¥¼ ê¸°ë°˜ìœ¼ë¡œ Jab4ì˜ 6-pass ê²€ìƒ‰ ìˆ˜í–‰
        search_results = pipeline.search(standalone_query, top_k=FINAL_TOP_K, eval_id=eval_id)
    
    if not search_results:
        if cached_ans: return cached_ans
        res = call_openai_with_retry([{"role": "system", "content": persona_chitchat}] + messages, model=QUERY_LLM_MODEL)
        ans_data = {"standalone_query": standalone_query, "topk": [], "answer": res.choices[0].message.content}
        answer_cache.set(m_hash, ans_data)
        return ans_data

    # Reuse answer if available, but update topk
    if cached_ans and cached_ans.get('answer'):
        print("(Reuse Answer Cache)", end=" ", flush=True)
        ans_text = cached_ans['answer']
    else:
        context = "\n".join([f"[Ref {i+1}] {h['_source']['content']}" for i, h in enumerate(search_results)])
        llm_msgs = [{"role": "system", "content": persona_qa}, {"role": "system", "content": f"ê´€ë ¨ ì •ë³´:\n{context}"}] + messages
        llm_msgs.append({"role": "user", "content": f"ë‹µë³€ ì „ ì§ˆë¬¸ ì¬í™•ì¸: '{standalone_query}'\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”."})
        res = call_openai_with_retry(llm_msgs, model=QUERY_LLM_MODEL)
        ans_text = res.choices[0].message.content

    ans_data = {
        "standalone_query": standalone_query,
        "topk": [h['_source']['docid'] for h in search_results],
        "answer": ans_text
    }
    answer_cache.set(m_hash, ans_data)
    return ans_data


def eval_rag(eval_filename: str, output_filename: str, include_references: bool = False):
    """
    RAG í‰ê°€ (v4.9.2_2)
    
    Args:
        include_references: Trueë©´ ë””ë²„ê¹…ìš© íŒŒì¼ë„ ìƒì„±
    """
    print(f"\n{'='*70}")
    print(f"RAG v4.9.2.jab6 - Robust Retrieval Recovery")
    print(f"{'='*70}")
    
    print("\n[CONFIG] ê°œì„ ì‚¬í•­:")
    print(f"  - âœ… v4.9.2_1 ê¸°ë°˜ (ì •ë³´ì§€ì‹ êµì‚¬)")
    print(f"  - ğŸ”´ SEARCH_OFF ë¡œì§ ê°œì„  (ê¸´ê¸‰)")
    print(f"  - âœ… ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´ ìš°ì„  ì²´í¬ ì¶”ê°€")
    print(f"  - âœ… ê³¼í•™ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥ (70ê°œ+)")
    print(f"  - âœ… ë³´ìˆ˜ì  ì ‘ê·¼: ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ê²€ìƒ‰")
    print(f"  - âœ… 5ê°œ ì˜¤ë¶„ë¥˜ ì§ˆë¬¸ ë³µêµ¬:")
    print(f"       1. ìˆ˜ì„±ì´ ëœ¨ê±°ìš´ ì´ìœ ëŠ”?")
    print(f"       2. í˜‘ê³¡ì´ í˜•ì„±ë˜ëŠ” ê³¼ì •ì€?")
    print(f"       3. ì¼ì‹ì´ ë°œìƒí•˜ëŠ” ì›ë¦¬ëŠ”?")
    print(f"       4. ë¦¬ë³´ì˜¤ì†œì˜ ì—­í• ì´ ë­ì•¼?")
    print(f"       5. í•´êµ¬ê°€ ìƒê²¨ë‚˜ëŠ” ì›ë¦¬ëŠ”?")
    print()
    
    # í‰ê°€ ë°ì´í„° ë¡œë“œ
    with open(eval_filename, 'r', encoding='utf-8') as f:
        eval_data = [json.loads(line) for line in f]
    
    print(f"[EVAL] í‰ê°€ ë°ì´í„°: {len(eval_data)}ê°œ\n")
    
    # í‰ê°€ ì‹¤í–‰
    # ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ ê²½ë¡œ
    checkpoint_path = f"{output_filename}.checkpoint"
    results = []
    start_idx = 0
    
    if os.path.exists(checkpoint_path):
        print(f"[INFO] ì²´í¬í¬ì¸íŠ¸ ë°œê²¬: {checkpoint_path}")
        with open(checkpoint_path, 'r', encoding='utf-8') as f:
            for line in f:
                results.append(json.loads(line))
        start_idx = len(results)
        print(f"[INFO] {start_idx}ë²ˆ ì§ˆë¬¸ë¶€í„° ì¬ê°œí•©ë‹ˆë‹¤.")

    for i, item in enumerate(eval_data[start_idx:], start_idx + 1):
        print(f"\n[EVAL] Test {i}/{len(eval_data)}")
        print(f"Question: {item['msg'][-1]['content'][:50]}...")
        
        try:
            response = answer_question(item["msg"], pipeline, include_references=include_references, eval_id=item["eval_id"])
            
            # ì œì¶œìš©: í•„ìˆ˜ í•„ë“œë§Œ
            result = {
                "eval_id": item["eval_id"],
                "standalone_query": response["standalone_query"],
                "topk": response["topk"],
                "answer": response["answer"]
            }
            
            # ë””ë²„ê¹…ìš©: references í¬í•¨
            if include_references:
                result["references"] = response.get("references", [])
            
            results.append(result)
            
            # ë§¤ ì‹œë„ë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            with open(checkpoint_path, 'a', encoding='utf-8') as f:
                f.write(f'{json.dumps(result, ensure_ascii=False)}\n')
        
        except Exception as e:
            print(f"[ERROR] Test {i} ì‹¤íŒ¨: {e}")
            # ì‹¤íŒ¨ ì‹œì—ë„ placeholder ì €ì¥ (ë‚˜ì¤‘ì— ìˆ˜ë™ ìˆ˜ì • ìš©ì´í•˜ë„ë¡)
            result = {
                "eval_id": item["eval_id"],
                "standalone_query": item["msg"][-1]["content"],
                "topk": [],
                "answer": f"ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}"
            }
            results.append(result)
            with open(checkpoint_path, 'a', encoding='utf-8') as f:
                f.write(f'{json.dumps(result, ensure_ascii=False)}\n')
    
    # ì œì¶œ íŒŒì¼ ìƒì„± (í•„ìˆ˜ í•„ë“œë§Œ)
    with open(output_filename, 'w', encoding='utf-8') as f:
        for result in results:
            submit_result = {
                "eval_id": result["eval_id"],
                "standalone_query": result["standalone_query"],
                "topk": result["topk"],
                "answer": result["answer"]
            }
            f.write(f'{json.dumps(submit_result, ensure_ascii=False)}\n')
    
    # ì™„ë£Œ ì‹œ ì²´í¬í¬ì¸íŠ¸ ì‚­ì œ
    if os.path.exists(checkpoint_path):
        os.remove(checkpoint_path)
    
    print(f"\n[SUCCESS] ì œì¶œ íŒŒì¼ ìƒì„±: {output_filename}")
    
    # ë””ë²„ê¹… íŒŒì¼ ìƒì„± (references í¬í•¨)
    if include_references:
        debug_filename = output_filename.replace('.csv', '_debug.jsonl')
        with open(debug_filename, 'w', encoding='utf-8') as f:
            for result in results:
                f.write(f'{json.dumps(result, ensure_ascii=False)}\n')
        print(f"[SUCCESS] ë””ë²„ê¹… íŒŒì¼ ìƒì„±: {debug_filename}")
    
    print(f"\n{'='*70}")
    print(f"[SUCCESS] í‰ê°€ ì™„ë£Œ!")
    print(f"  - v4.9.2_1 ë¬¸ì œ: 27ê°œ ì¤‘ 5ê°œ ì˜¤ë¶„ë¥˜ (18.5% ì˜¤ë¥˜ìœ¨)")
    print(f"  - v4.9.2_2 ê°œì„ : 5ê°œ ì§ˆë¬¸ ë³µêµ¬ â†’ ì •í™•ë„ í–¥ìƒ")
    print(f"  - v4.9.2_2 ì˜ˆìƒ: MAP 0.8977+ (+0.5~1.5%p)")
    print(f"{'='*70}")


# ============================================================================
# Main (v4.9.2_1)
# ============================================================================

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='RAG v4.9.2_2 - SEARCH_OFF ë¡œì§ ê°œì„ ')
    parser.add_argument('--competition_mode', action='store_true', default=True,
                        help='Competition mode')
    parser.add_argument('--no_competition_mode', dest='competition_mode', action='store_false',
                        help='Disable competition mode')
    parser.add_argument('--mode', choices=['index', 'infer', 'full'], default='full',
                        help='ì‹¤í–‰ ëª¨ë“œ: index=ì¸ë±ì‹±ë§Œ, infer=ì¶”ë¡ ë§Œ, full=ì „ì²´')
    parser.add_argument('--index-name', default='rag_v4922',
                        help='Elasticsearch ì¸ë±ìŠ¤ ì´ë¦„')
    parser.add_argument('--force-reindex', action='store_true',
                        help='ì¸ë±ìŠ¤ë¥¼ ê°•ì œë¡œ ì‚­ì œ í›„ ì¬ìƒì„±')
    parser.add_argument('--include-references', action='store_true',
                        help='ë””ë²„ê¹…ìš© references í•„ë“œ í¬í•¨')
    parser.add_argument('--eval-file', default='../data/eval.jsonl',
                        help='í‰ê°€ ë°ì´í„° íŒŒì¼')
    parser.add_argument('--output-file', default='submission_v4.9.2_jab4.csv',
                        help='ì œì¶œ íŒŒì¼ ì´ë¦„')
    parser.add_argument('--log-path', default=None,
                        help='ë¡œê·¸ íŒŒì¼ ê²½ë¡œ (ì„ íƒ)')
    
    args = parser.parse_args()
    logger = setup_logger(args.log_path)
    
    print("\n" + "="*70)
    print("RAG v4.9.2.jab6 - Retrieval & Logic Hardening")
    print("="*70)
    
    print("\n[v4.9.2_2 ê°œì„ ì‚¬í•­]")
    print("ğŸ”´ ê¸´ê¸‰ ìˆ˜ì •: SEARCH_OFF ë¡œì§ ê°œì„ ")
    print("âœ… 1. ê³¼í•™ ì§ˆë¬¸ íŒ¨í„´ ìš°ì„  ì²´í¬ (ì´ìœ , ì›ë¦¬, ê³¼ì •, ì—­í•  ë“±)")
    print("âœ… 2. ê³¼í•™ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥ (70ê°œ+)")
    print("     - í–‰ì„±: ìˆ˜ì„±, ê¸ˆì„±, í™”ì„±, ëª©ì„±, í† ì„±...")
    print("     - ì§€í˜•: í˜‘ê³¡, í•´êµ¬, ì‚°ë§¥, ë¶„ì§€...")
    print("     - ì²œë¬¸: ì¼ì‹, ì›”ì‹, ì¡°ì„, ê³µì „, ìì „...")
    print("     - ì„¸í¬: ë¦¬ë³´ì†œ, ë¯¸í† ì½˜ë“œë¦¬ì•„, ì—½ë¡ì²´...")
    print("âœ… 3. ë³´ìˆ˜ì  ì ‘ê·¼: ì˜ì‹¬ìŠ¤ëŸ¬ìš°ë©´ ê²€ìƒ‰")
    print("âœ… 4. 5ê°œ ì˜¤ë¶„ë¥˜ ì§ˆë¬¸ ë³µêµ¬:")
    print("     - ìˆ˜ì„±ì´ ëœ¨ê±°ìš´ ì´ìœ ëŠ”?")
    print("     - í˜‘ê³¡ì´ í˜•ì„±ë˜ëŠ” ê³¼ì •ì€?")
    print("     - ì¼ì‹ì´ ë°œìƒí•˜ëŠ” ì›ë¦¬ëŠ”?")
    print("     - ë¦¬ë³´ì˜¤ì†œì˜ ì—­í• ì´ ë­ì•¼?")
    print("     - í•´êµ¬ê°€ ìƒê²¨ë‚˜ëŠ” ì›ë¦¬ëŠ”?")
    
    print(f"\n[ì‹¤í–‰ ëª¨ë“œ]")
    print(f"  - Mode: {args.mode}")
    print(f"  - Index: {args.index_name}")
    print(f"  - Force Reindex: {args.force_reindex}")
    print(f"  - Include References: {args.include_references}")
    print()
    
    # Pipeline ì´ˆê¸°í™”
    print("[MAIN] Pipeline ì´ˆê¸°í™”...", flush=True)
    pipeline = RAGPipelineV4922(es, index_name=args.index_name)
    
    # ì¸ë±ì‹± ëª¨ë“œ
    if args.mode in ['index', 'full']:
        print("\n[MODE] Indexing mode")
        
        # ì¸ë±ìŠ¤ ìƒì„±
        pipeline.create_index(settings, mappings, force=args.force_reindex)
        
        # ë¬¸ì„œ ë¡œë“œ
        print("[INFO] ë¬¸ì„œ ë¡œë“œ ì¤‘...", flush=True)
        with open("../data/documents.jsonl", 'r', encoding='utf-8') as f:
            documents = [json.loads(line) for line in f]
        
        print(f"[INFO] ì´ {len(documents)}ê°œ ë¬¸ì„œ")
        
        # ì¸ë±ì‹±
        print("\n[INFO] ì¸ë±ì‹± ì‹œì‘...", flush=True)
        start_time = time.time()
        pipeline.index_documents(documents, batch_size=BATCH_SIZE)
        elapsed = time.time() - start_time
        print(f"[INFO] ì¸ë±ì‹± ì™„ë£Œ: {elapsed:.1f}ì´ˆ ({elapsed/60:.1f}ë¶„)")
    
    # ì¶”ë¡  ëª¨ë“œ
    if args.mode in ['infer', 'full']:
        print("\n[MODE] Inference mode")
        
        # ì¸ë±ìŠ¤ ì¡´ì¬ í™•ì¸
        if not es.indices.exists(index=args.index_name):
            print(f"[ERROR] ì¸ë±ìŠ¤ '{args.index_name}'ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤!")
            print(f"[INFO] ë¨¼ì € '--mode index'ë¡œ ì¸ë±ì‹±ì„ ì‹¤í–‰í•˜ì„¸ìš”.")
            exit(1)
        
        # í‰ê°€ ì‹¤í–‰
        eval_rag(args.eval_file, args.output_file, include_references=args.include_references)
    
    print("\n[DONE] v4.9.2_2 ì‹¤í–‰ ì™„ë£Œ! ğŸ‰")
    print("\n[ì˜ˆìƒ ì„±ëŠ¥]")
    print("  - v4.9.2_1: 27ê°œ ì¤‘ 5ê°œ ì˜¤ë¶„ë¥˜ (18.5% ì˜¤ë¥˜ìœ¨)")
    print("  - v4.9.2_2: 5ê°œ ì§ˆë¬¸ ë³µêµ¬ â†’ MAP 0.8977+ ì˜ˆìƒ")
    print("  - ê°œì„ : SEARCH_OFF ë¡œì§ ê°œì„  (+0.5~1.5%p)")


